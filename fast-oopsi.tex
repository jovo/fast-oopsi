\documentclass[12pt]{article}
\input{/Users/joshyv/Research/misc/latex_paper.tex} 
\newcommand{\hC}{\widehat{C}}
\newcommand{\hn}{\widehat{n}}
\newcommand{\zzz}{z}
\newcommand{\xT}{\ve{C}}
\newcommand{\yT}{\ve{y}}
\newcommand{\nT}{\ve{n}}
\newcommand{\zT}{\ve{n}}
\newcommand{\FT}{\ve{F}}
\newcommand{\lT}{\ve{\lam}}
\newcommand{\wX}{\widehat{\ve{C}}}
\newcommand{\wY}{\widehat{\ve{Y}}}
\newcommand{\CaT}{\Cav}
\newcommand{\ax}{\argmax_{\ve{C}_t \geq 0 \forall t}}
\newcommand{\an}{\argmin_{n_t \geq 0 \forall t}}
\newcommand{\az}{\argmin_{\bM \bC - \bb \geq \ve{0}}}
\newcommand{\ath}{\argmax_{\bth \in \ve{\Theta}}}
\newcommand{\ann}{\argmin_{n_t \in \mathbb{N}_0 \forall t}}
\newcommand{\hnm}{\widehat{\bn}}
\newcommand{\hCm}{\widehat{\bC}}
%\newcommand{\hbm}{\widehat{\ve{\nu}}}
%\newcommand{\hbn}{\widehat{\bn}}
%\newcommand{\hbC}{\widehat{\bC}}

\lhead{Vogelstein JT, et al}
\rhead{Fast spike train inference from calcium imaging}
%\headheight=14.5pt


\title{Fast spike train inference from calcium imaging}

\author{
Joshua T.~Vogelstein% \thanks{ Use footnote for providing further information about author (webpage, alternative address)---\emph{not} for acknowledging funding agencies.}
% \\ Department of Neuroscience\\
%Johns Hopkuns University\\
%Baltimore, MD 21205 \\
%\texttt{joshuav@jhu.edu} \\
%\And
%Baktash Babadi\\
%Department of Neuroscience \\
%Columbia University \\
%New York City, NY, 10027 \\
%\texttt{bb2280@columbia.edu} \\
%\And
%Brendon O.~Watson\\
%Department of Neuroscience \\
%Columbia University \\
%New York City, NY, 10027 \\
%\texttt{bow4@columbia.edu} \\
%\And
%Rafael Yuste\\
%Department of Neuroscience \\
%Columbia University \\
%New York City, NY, 10027 \\
%\texttt{rmy5@columbia.edu} \\
%\And
, other fuckers, Liam Paninski\\
%Department of Statistics\\
%Columbia University \\
%New York City, NY, 10027 \\
%\texttt{liam@stat.columbia.edu} \\
}

\begin{document}

\maketitle

\begin{abstract}
%Experiments often yield measurements of variables that are naturally constrained to be nonnegative. In such scenarios, it may be desirable to filter (or deconvolve) the observations to find the most likely trajectory of the nonnegative variable, given the noisy observations. Here, we develop a computationally-efficient optimal filter for a certain subset of nonnegatively constrained deconvolutions.  Specifically, for any nonnegative variable that is filtered by a matrix linear differential equation and observed with independent, log-concave noise, we can infer the optimal nonnegative trajectory via straightforward interior-point methods in $O(T)$ (linear) time, as opposed to more standard approaches requiring $O(T^3)$ time (where $T$ is the total number of time steps).  The key is to make use of the tridiagonal structure of the Hessian of the log-posterior here, which allows us to perform each Newton iteration in linear time.  We apply this filter to an important problem in neuroscience: inferring a spike train from noisy calcium fluorescence observations. We demonstrate the filter's improved performance on simulated and real data. In conclusion, we propose that this filter is readily applicable for a number of real-time applications, including spike inference from simultaneously-observed large neural populations.
\end{abstract}

\section{Introduction}

%Experiments often yield measurements of variables known to be nonnegative, due to physical constraints. Examples arise in a wide variety of fields, including both audio and image signal processing. Therefore, determining the most likely trajectory of the nonnegative variable, given the observations,  requires a filter that performs a nonnegative deconvolution. Furthermore, by imposing such a nonnegativity constraint on the solution, these constraints also regularize the resulting inference \cite{LeeSeung99, LeeSeung01, HuysPaninski06}, by combating against ``ringing'' overfitting effects. Nonnegative deconvolution is closely related to another set of problems, called nonnegative matrix factorization, which decomposes a nonnegative matrix into the product of two nonnegative matrices \cite{OGradyPearlmutter06}.
%
%Unfortunately, imposing nonnegative constraints on deconvolution or matrix factorization remains difficult.  Although a number of methods have recently been introduced \cite{PortugalVicente94, LeeSeung99, LeeSeung01, LeeNg06}, they all scale polynomially with the number of time steps.  Here, we show that in some important special cases, we can solve these problems in linear time.  The major restriction we place on the filter is that it is the solution of a matrix linear ordinary differential equation. 
%
%One important example comes from neuroscience.  Calcium-sensitive fluorescent indicators are becoming an increasingly popular tool for visualizing neural spiking activity, both in vivo and in vitro \cite{YasudaSvoboda04}. Spikes cause a sharp rise in intracellular calcium concentration, $\Ca$, which is reported by a concurrent rise in fluorescence activity \cite{YasudaSvoboda04}.  As spikes are nonnegative entities, and $\Ca$ and spikes are related by a linear matrix differential equation, this data is indeed a special case of the more general class of problems described above.  We therefore develop an optimal and efficient algorithm for inferring spikes from noisy fluorescence observations, which we cast in the language of a nonnegative deconvolution problem. 
%
%The remainder of this paper is organized as follows. In Section \ref{sec:methods} we describe a highly efficient and optimal nonnegative filter for solving the kinds of problems described above, by making use of two tools. First, we use an interior-point technique for dealing with the nonnegativity constraint, in which we iteratively solve a series of related log-concave problems. Second, when the likelihood to be maximized adheres to our constraints, we can use an efficient algorithm to solve each iteration in $O(T)$ time, where $T$ is the total number of time steps. In Section \ref{sec:results}, we first use simulations to compare this filter with a few other possible filters: the optimal linear (i.e., Wiener) filter, and a fast version of an algorithm referred to as projection pursuit regression (PPR), adapted to our model of interest.  Then, we apply these fast filters to an example fluorescence time-series recorded from a live neuron \emph{in vitro}, and compare with an optimal nonlinear particle filter \cite{BJ08}. Finally, in Section \ref{sec:dis}, we discuss some applications and extensions.
%
%
%As mentioned above, using calcium-sensitive fluorescent indicators is becoming increasingly popular, as it enables simultaneously observing the activity of many (e.g., up to $500$) neurons.  Unfortunately, the image quality is typically relatively poor. To maximize the utility of this data, it would therefore be desirable to have an optimal filter, that would provide a spike train given only fluorescence measurements.  Recent work towards inferring spike trains from such data have made significant advances \cite{SmettersYuste99, KerrHelmchen05, YaksiFriedrich06, HolekampHoly08}, but none have utilized the fact that spikes are nonnegative. We therefore develop such a filter, in the larger context of nonnegative filter theory.

%\paragraph{Constructing the optimal nonnegative filter} \label{sec:nng}

%\section{Model} \label{sec:model}
\section{Methods} \label{sec:methods}

\subsection{Model}

We assume the following discrete-time state-space model relating spikes, $n_t$, baseline subtracted intracellular calcium concentration, denoted by $C_t$, and fluorescence measurements, $F_t$: (1) spikes follow Poisson statistics with rate $\lam \Del$, (2) $C_t$ decays exponentially with decay $\gamma$ to baseline $\nu$, but jumps by $\rho$ after each spike, and (3) fluorescent observations are linear functions of $C_t$ with additive Gaussian noise with variance $\sig^2$.  Together, these assumptions imply the following model:

\begin{align}
F_t &= \alpha C_t + \beta +  \sig \varepsilon_t, \qquad &\varepsilon_t \sim \mathcal{N}(0,1) \label{eq:obs} \\
C_t  &= \gamma  C_{t-1} + \nu + \rho n_t,  \qquad &n_t \sim \text{Poisson}(n_t; \lam \Del) \label{eq:trans}  
\end{align}

\noindent where $\alpha$ and $\beta$ set the baseline and offset respectively. To enforce identifiability, we will let $\alpha=1$ and $\beta=0$, without loss of generality, as explained below.

\subsection{Inference} \label{sec:inf}

Given such a model, our goal is to find the maximum \emph{a posteriori} (MAP) spike train, i.e., the most likely spike train, $\bn=[n_1,\ldots, n_T]$, where $T$ is the final time step,  given the fluorescence measurements, $\bF=[F_1,\ldots, F_T]$. Thus, the objective function that we'd like to solve may be written as:

%\begin{subequations}
\begin{align}
\hnm &=  \ann P(\bn | \bF) = \ann P(\bF | \bn) P(\bn) \nonumber  \\
&= \argmax_{n_t \in \mathbb{N}_0 \forall t} \prod_{t=1}^T  P(F_t | C_t)  P(n_t) = \argmax_{n_t \in \mathbb{N}_0 \forall t} \sum_{t=1}^T \big( \log P(F_t | C_t) + \log P(n_t)\big)  \nonumber \\
&= \ann  \sum_{t=1}^T \bigg( \frac{1}{2 \sig^2}(F_t - C_t)^2  -  n_t \log \lam \Del + \log n_t! \bigg),   \label{eq:obj}
\end{align}
%\end{subequations}

\noindent where $\mathbb{N}_0 = \{0, 1, 2, \ldots\}$, and $C_t$ is implicitly a function of $n_t$.  Unfortunately, the computational complexity of solving Eq. \eqref{eq:obj} scales exponentially with $T$, i.e., is $O($e$^T)$, making Eq. \eqref{eq:obj} an NP-hard problem.  Thus, instead of an exact solution, we propose to make an approximation that reduces the complexity to be \emph{polynomial} in $T$.  In particular, we relax the assumption that we must have an integer number of spikes at any time step, by approximating the Poisson distribution with an exponential.  Note that this is a common approximation technique in the machine learning literature \cite{HastieFriedman01}, as it is the closest convex relaxation to its non-convex counterpart.  The constraint on $n_t$ in Eq. \eqref{eq:obj} is therefore relaxed from  $n_t \in \mathbb{N}_0$ to $n_t \geq 0$:

\begin{align} \label{eq:obj2}
\hnm &\approx \an  \sum_{t=1}^T \left( \frac{1}{2 \sig^2}(F_t - C_t)^2  +  n_t  \lam \Del \right).
\end{align}

While this convex relaxation makes the problem tractable, the ``sharp'' threshold imposed by the nonnegativity constraint prohibits the use of standard gradient ascent techniques \cite{CONV04}. We therefore take an ``interior-point'' (or ``barrier'') approach, in which we drop the sharp threshold, and add a barrier term, which must approach $-\infty$ as $n_t$ approaches zero (e.g., $-\log n_t$) \cite{CONV04}.  By iteratively reducing the weight of the barrier term, $\zzz > 0$, we are guaranteed to converge to the correct solution \cite{CONV04}, $\hnm$.  Thus, our goal is to efficiently solve:

\begin{align} \label{eq:eta}
\hbn_{\zzz} &= \argmin_{n_t \forall t}  \sum_{t=1}^T \left( \frac{1}{2 \sig^2}(F_t - C_t)^2  +  n_t  \lam \Del - \zzz \log(n_t) \right),
\end{align}

\noindent which is concave and may therefore be solved exactly and efficiently using a gradient ascent technique. To see how, we first note that   
% such as Newton-Raphson, which, na\"{i}ely, requires inverting the Hessian (second derivative), an $O(T^3)$ operation.  
%We have therefore reduced the computational burden from exponential to polynomial in $T$.   However, by utilizing the structure of Eq. \eqref{eq:trans}, we can devise an algorithm that further reduces computation burden from polynomial in $T$ to \emph{linear} in $T$.  In particular, we note here that 
spikes and calcium are related to one another via a simple linear transformation, namely, $n_t=f(C_t,C_{t-1})= (C_t - \gamma C_{t-1} - \nu) / \rho$, or, in matrix notation: 

\begin{align} \label{eq:M}
\ve{M} \bC - \bb=
\begin{bmatrix}
1/\rho & 0  & 0 & \cdots & \cdots \\
1/\rho & -\gamma/\rho & 0 & \cdots & \cdots \\
0 & 1/\rho & -\gamma/\rho & 0 & \cdots  \\
\vdots & \vdots & \vdots & \vdots & \vdots  \\
0 & 0 & 0 & 1/\rho & -\gamma/\rho
\end{bmatrix}
\begin{bmatrix}
C_1 \\ C_2 \\ \vdots \\ \vdots \\ C_T  
\end{bmatrix}
-\begin{bmatrix}
\nu/\rho \\ \nu/\rho \\ \vdots \\ \vdots \\ \nu/\rho 
\end{bmatrix}
= 
\begin{bmatrix}
n_1 \\ n_2 \\ \vdots \\ \vdots \\ n_T
\end{bmatrix}
= \bn
\end{align}

\noindent where $\ve{M} \in \mathbb{R}^{T \times T}$ is a bidiagonal matrix,  and $\bb$, $\bC=[C_1,\ldots, C_T]$, and $\bn$ are $T$ dimensional column vectors. Given Eq. \eqref{eq:M}, we may rewrite Eq. \eqref{eq:eta} in terms of $\bC$:

%\begin{subequations}  
\begin{align} 
\hbC_{\zzz} &= \argmin_{(C_t - \gamma C_{t-1} - \nu)/\rho \geq 0 \forall t} \sum_{t=1}^{T} \left( \frac{1}{2 \sig^2}(F_t - C_t)^2  + (C_t - \gamma C_{t-1} - \nu)  \lam \Del/\rho - \zzz \log[(C_t - \gamma C_{-1} -\nu )/\rho] \right) \nonumber \\
&= \az  \frac{1}{2 \sig^2} \norm{\bF - \bC}^2 + \lam \Del (\bM \bC-\bb )' \ve{1}  - \zzz \log(\bM \bC - \bb)'\ve{1},  \label{eq:eta2}
\end{align}
%\end{subequations}

\noindent where $\bM \bC -\bb \geq \ve{0}$ indicates that every element of $\bM \bC -\bb$ is greater than or equal to zero, $'$ indicates transpose, and  $\ve{1}$ is a $T$ dimensional column vectors. As \eqref{eq:eta2} is concave (it is identical to Eq. \eqref{eq:eta} with different notation), we may use any descent technique to find $\hbC_{\zzz}$.  We elect to use the Newton-Raphson approach: 

%, i.e., update $\widehat{\ve{C}}_{\zzz}$ by adding to it the solution to $\ve{H}\ve{C} = \ve{g}$, weighted by $0<s\leq1$, where $\ve{H}$ and $\ve{g}$ are the Hessian and gradient of the argument in \eqref{eq:goal2}.  The step size, $s$, ensures that the posterior converges, by enforcing an increase in the objective with each step. Therefore, we have: 

\begin{subequations} \label{eq:NR}
\begin{align}
\hbC_{\zzz} &\leftarrow \hbC_{\zzz} + s \bd \\
\bH \bd &= \bg \\
\ve{g} &= -\frac{1}{\sig^2}(\bF - \hbC_{\zzz}) + \lam \Del \ve{M}' \ve{1} - \zzz \ve{M}' (\ve{M} \hbC_{\zzz} -\bb)^{-1} \label{eq:g} \\
\ve{H} &= \frac{1}{\sig^2} \ve{I} + 2 \zzz \ve{M}' (\ve{M} \hbC_{\zzz} - \bb)^{-2} \ve{M} \label{eq:H}
\end{align}
\end{subequations}

\noindent where $s$ is the step size, $\bd$ is the step direction, and $\bg$ and $\bH$ are the gradient (first derivative) and Hessian (second derivative) of the likelihood, respectively (the likelihood is the argument to be minimized in Eq. \eqref{eq:eta2}). Note that we use ``backtracking linesearches'', meaning that for each iteration, we find the maximal $s$ that is between $0$ and $1$ and decreases the likelihood.

Typically, implementing Newton-Raphson requires inverting the Hessian, i.e., $\bd = \bH^{-1} \bg$, a computation consuming $O(T^3)$ time. Instead, because $\ve{M}$ is bidiagonal, the Hessian is tridiagonal, so the solution may be found in $O(T)$ time via standard banded Gaussian elimination techniques (which can be implemented efficiently in Matlab using $\bH \backslash \bg$). The resulting fast algorithm for solving the optimization problem in \eqref{eq:obj2} is the main result of this paper, i.e.  we may approximately infer the optimal solution  for models characterized by equations such as \eqref{eq:obs} and \eqref{eq:trans} in linear time, whereas an exact solution would require exponential time, and a na\"{i}ve approximate solution would require polynomial time.  


\subsection{Learning} \label{sec:est}

In the above, we assumed that the parameters governing our model, $\vth=\{\alpha, \beta, \sig, \gam, \nu, \rho. \lam\} \in \ve{\Theta}$,  %=\Real^5_+$ (because all these parameters are necessarily non-negative)
were known. In general, however, these parameters may be estimated from the data. To find the maximum likelihood estimator for the parameters, $\hvth$, we must integrate over the unknown variable, $\zT$. However, integrating over all possible spike trains is typically approximated by Monte Carlo approaches, which is relatively slow. Thus, one often approximates: % the integral with the most likelihood estimate of $\zT$ \cite{??}, yielding:

%\begin{subequations} \label{eq:par}
\begin{align} \label{eq:par1}
\widehat{\bth} &= \argmax_{\bth \in \ve{\Theta}} \int P(\ve{F}|\ve{n}, \ve{\theta}) P(\ve{n} | \ve{\theta} ) d\bn \approx \argmax_{\bth \in \ve{\Theta}}P(\ve{F}| \hnm, \ve{\theta}) P(\hnm | \ve{\theta})
\end{align}

\noindent where $\hnm=[\hn_1, \ldots, \hn_T]$ is estimate of $\hnm$ from the inference algorithm described above ($\hCm$ is defined similarly). The approximation in \eqref{eq:par} is good whenever the likelihood is very peaky, meaning that most of the mass is around the MAP sequence.\footnote{The approximation in \eqref{eq:par} may be considered a first-order Laplace approximation}  In particular, for state-space models, one often approximates the integral in \eqref{eq:par} with the Viterbi path, i.e., the MAP path of the hidden states \cite{Rabiner89}. Finding the Viterbi path is often far easier than solving the integral in \eqref{eq:par}, as in the case here.  Due to the state space nature of the above model (Eqs \eqref{eq:obs} and \eqref{eq:trans}), the optimization in Eq \eqref{eq:par1} simplifies significantly.  More specifically, the likelihood term, $P(\bF | \hnm, \bth)$ may be written as:

\begin{align} 
P(\bF | \hnm, \bth) &= \prod_{t=1}^T P(F_t | n_t, \bth) = \prod_{t=1}^T \frac{1}{\sqrt{2 \pi \sig^2}} \exp \left\{-\frac{1}{2\sig^2} (F_t - \alpha \hC_t - \beta )^2\right\} \nonumber \\
&= \prod_{t=1}^T \frac{1}{\sqrt{2 \pi \sig^2}} \exp \left\{-\frac{1}{2\sig^2} \big[F_t - \alpha (\gamma \hC_{t-1} + \nu + \rho n_t) - \beta \big]^2\right\} \label{eq:lik}
\end{align}

\noindent which follows from Eq \eqref{eq:trans} because $\hC_t$ is deterministic given $\hn_t$. Considering for a moment, only the quadratic term in Eq \eqref{eq:lik}, the terms can be reorganized:

\begin{align}
F_t - \alpha (\gamma \hC_{t-1} + \nu + \rho \hn_t) - \beta &= F_t - (\alpha \gamma) \hC_{t-1} - (\alpha \nu + \beta) - (\alpha \rho) \hn_t 
\end{align}

\noindent which emphasizes the over-parameterization in our model.  Thus, without loss of generality, we can assume that $\alpha=1$ and $\beta=0$. Taking logs in Eq \eqref{eq:lik} then yields:

\begin{align} 
\log P(\bF | \hnm, \bth) &=  -\frac{T}{2} \log (2 \pi \sig^2)  - \frac{1}{2\sig^2} \sum_{t=1}^T (F_t - \gamma \hC_{t-1} - \nu - \rho \hn_t)^2 \nonumber \\
&=  -\frac{T}{2} \log (2 \pi \sig^2)  - \frac{1}{2\sig^2} \norm{\bF- \hbX \ve{\eta}}^2
 \label{eq:lik2}
\end{align}

\noindent where  $\ve{\eta}=[\gam, \, \nu, \, \rho]$, $\hbX=[\hbX_1, \ldots, \hbX_T]'$, and $\hbX_t=[\hC_{t-1}, \, 1, \, \hn_t]$.  Similarly, taking the log of the prior term, $P(\hnm | \bth )$ yields:

\begin{align} \label{eq:prior}
\log P(\hnm | \bth) =  \sum _{t=1}^T \log(\lam \Del)  -\lam \Del \hn_t =T (\lam \Del) - \lam \Del \hnm' \ve{1} 
\end{align}

\noindent Plugging Eqs \eqref{eq:lik2} and \eqref{eq:prior} back into Eq \eqref{eq:par1}, and noting that log is a monotonic function, we have:


%Given Eqs. \eqref{eq:obs} and \eqref{eq:trans}, we can write the above likelihood function as:

\begin{align} 
%\hvth &=\argmax_{\vth \in \ve{\Theta}}  \sum_{t=1}^T \left(- \frac{1}{2} \log (2\pi\sig^2)-\frac{1}{2\sig^2} (F_t - \gamma \hC_{t-1} -\nu  - \rho \hn_t)^2 \right) + \sum_{t=1}^T \big(\log (\lam \Del) -  \lam \Del n_t\big) \nonumber \\
\hvth &=\argmax_{\vth \in \ve{\Theta}} - \frac{T}{2} \log (2\pi\sig^2) - \frac{1}{2\sig^2} \norm{\bF - \hbX \ve{\eta}}_2^2 + T \log (\lam \Del) - \lam \Del \hnm' \ve{1} \label{eq:par} 
%&=\argmin_{\vth}  \frac{T}{2} \log (2\pi\sig^2)  +  \frac{1}{2\sig^2} \norm{\bY + \ve{\eta} \bX}_2^2 + T \log (\lam \Del) - \lam \Del \bn' \ve{1} \label{eq:par} 
\end{align}
%\end{subequations}

\noindent %where %$\bY=\bF- \hnm$, 
%$\bX=[\hCm, \,  \ve{1},\,  \hnm]'$, $\ve{\eta}=[\gamma, \nu, \rho]'$, and $\hCm=[\hC_1, \hC_2, \ldots, \hC_{T-1}]'$ is the inferred calcium concentration using the algorithm described in Section \ref{sec:inf} (and $\hnm$ is defined in a similar fashion). Note that had we not fixed $\alpha=1$ and $\beta=0$, then we would have had $\ve{\eta}=[\alpha \gamma, \alpha \nu + \beta, \alpha \nu]$, and then these parameters would not have been identifiable.  
Estimating the parameters then separates into three log-concave problems.  In particular, solving for $\widehat{\ve{\eta}}$ is simply:

\begin{align} 
\widehat{\ve{\eta}} &= \argmin_{\ve{\eta},\: 0< \eta_1<1} \frac{1}{2} \norm{\bF - \hbX \ve{\eta}}_2^2 
=  \argmin_{\ve{\eta},\: 0< \eta_1<1} \frac{1}{2} \ve{\eta}' \bQ  \ve{\eta} - \bL'  \ve{\eta} \label{eq:m} 
\end{align}

\noindent where $\bQ=\hbX' \hbX$, and $\bL=\hbX' \bF$, and the constraint, $0<\eta_1<1$, ensures that the time constant is both non-negative and finite.  Eq \eqref{eq:m} may be solved using standard quadratic optimization tools, such as Matlab's \texttt{quadprog}.  

Unfortunately, the recursive nature of our algorithm makes solving for $\hbeeta$ in this manner unidentifiable. In particular, because both $\hbX$ and $\beeta$ can scale and shift $\bC$, alternating between inferring $\hbX$ and $\hbeeta$ does not converge (data not shown). However, this is not particularly problematic, because the fluorescence measurements are in arbitrary units. This unit arbitrariness suggests that $\bF$ may be scaled and shift without loss of generality. The prior term and non-negativity constraint on $\bn$, however, imposes small costs associated with scaling and shifting $\bn$ and $\bC$.  We therefore estimate $\beeta$ from the raw fluorescence trace.  More specifically, to estimate $\gamma$, we manually find a fluorescence sequence that seem to be decaying, estimate the time constant $\tau$, and let $\gamma=1-\Del/\tau$.  For $\nu$, we manually find a fluorescence sequence that seems to be near baseline, label it $[F_s,\ldots, F_t]$, and let $\nu=\frac{1}{t-s}\sum_{u=s}^t F_u$.  Finally, we estimate $\rho$ by letting it be equal to what we manually determine it be, by eye.  In practice, we have found that by first scaling and shifting $\bF$ to be between $0$ and $1$, subsequent traces from the same or different cells have very similar values for $\beeta$, and therefore, these parameters need not be tweaked much.  Furthermore, the effective signal-to-noise ratio (SNR) of the inferred spike train does not depend strongly on these parameters, in agreement with previous results \cite{YaksiFriedrich06}.  The other parameters, $\sig$ and $\lam$, can be solved for analytically:

\begin{align} 
\widehat{\sig}^2 &= \frac{1}{T} \norm{\bF - \hbX \widehat{\ve{\eta}}}_2^2 \label{eq:sig}\\
\widehat{\lam} &=  \frac{1}{T \Del} \hnm' \ve{1} \label{eq:lam}
\end{align}

%\noindent where Eq. \eqref{eq:m} may be solved using standard linearly constrained quadratic programming (the constraint ensures that the time constant is both non-negative and finite), and the other two may be solved analytically. Importantly, when $\bn$ is unknown, $\widehat{\ve{\eta}}$ is not identifiable, as $\bY$ and $\ve{\eta}$ can both modify the offset and scale of $\bn$. Thus, we estimate $\ve{\eta}$ \emph{before} doing the analysis, and then hold it fixed throughout.  

%. Because the noise is Gaussian, $a$ and $\beta$ may be estimated using standard constrained quadratic programming:
% 
%\begin{subequations}
%\begin{align} \label{eq:m}
%\{\ha,\hbeta\}% &= \argmax_{a,\beta>0}\frac{1}{\sqrt{2 \pi} \sig} \exp \left\{ -\frac{1}{2} \left(\frac{F_t - a \widehat{C}_{t-1} - \widehat{n}_t - \beta}{\sig}\right)^2\right\} 
%= \argmin_{a,\beta>0} \sum_{t=1}^T (F_t - a \widehat{C}_{t-1} - \widehat{n}_t -\beta )^2 \\
%\widehat{\ve{\eta}} &= \argmin_{\bm>0} \norm{\bY + \bm \bX}_2^2. % = \argmin_{\bm>0}  \bm' \bX' \bX \bm  - 2 \bY' \bX \bm 
%\end{align}
%\end{subequations}
%
% Similarly, because $\varepsilon_t$ is Gaussian, we compute $\widehat{\sig}$ analytically using: 
%
%\begin{align}
%\widehat{\sig}^2 = \frac{1}{T} \norm{\bY + \widehat{\ve{\eta}} \bX}_2^2.
%\end{align}
%
%\noindent The parameter for the prior term is given by:
%
%\begin{align}
%\widehat{\lam} = \argmax_{\lam} P(\widehat{\ve{n}} | \lam) =  \argmax_{\lam} \log P(\widehat{\ve{n}} | \lam) =  \frac{T}{ \Del \bn' \ve{1}}
%\end{align}
%
%\noindent which follows from assuming that $p(\zT | \lT)$ is exponential in \eqref{eq:approx}.

\section{Results}

\subsection{Main Result}

To evaluate the efficacy of our fast filter, we compare it with the optimal linear (i.e., Wiener) filter \cite{Wiener49}. The top three panels of Fig. \ref{fig:schem} depict a typical dataset simulated according to our model, Eqs. \eqref{eq:obs} and \eqref{eq:trans}. Beneath the simulation, we show both the Wiener filter output (fourth panel) and our fast filter output (bottom panel).  For both filters, we provided only the fluorescence observations depicted in the top panel, and $\ve{\eta}$.  From this data, we estimate the remaining parameters, and infer the hidden spike train. Several differences between these two approaches should be apparent.  First, the effective signal-to-noise ratio (SNR) of our fast filter improves upon the optimal linear filter.  Second, while the Wiener filter induces a ``ringing'' effect (where the inferred signal oscillates above and below zero), our fast filter completely eliminates this effect.  These two improvements are common observations upon imposing a non-negative constraint when appropriate \cite{ShumwayStoffer06}.  Importantly, both our implementation of the Wiener filter and our fast filter require only $O(T)$ time, whereas the na\"{i}ve implementation of the Wiener filter requires $O(T \log(T))$, and the na\"{i}ve implementation of a non-negative filter requires $O(T^3)$.  Thus, when our approximation in Eq \eqref{eq:obj2} is good, our filter outperforms the Wiener filter, and imposes approximately the same computational burden.

\begin{figure}
\centering \includegraphics[width=.9\linewidth]{schem}
\caption{Simulation demonstrating our neuron model and inference. Note that the effective signal-to-noise ratio (SNR) of our fast filter improves on the Wiener filter, largely by eliminating the ringing effect present in the Wiener filter output.  For both filters, only the fluorescence was provided to the algorithm, so both the parameters and the inference were performed using this minimal amount of data.  Top panel: Simulated fluorescence. Second panel: Simulated intracellular calcium concentration. Third panel: Simulated spike train.  Fourth panel: Wiener filter (blue for positive inference, red for negative), superimposed on simulated spike train (black triangles).  Bottom panel: Fast filter (same conventions as fourth panel). Parameters:  $\gamma=0.94$, $\nu=0$, $\rho=1$, $\sig=0.3$, $\lam=8$, $\Del=0.005$ msec.} \label{fig:schem}
\end{figure}

\subsection{Fast Spiking Neuron}

When the observed neuron is spiking quickly, the Poisson distribution may be well approximated by a Gaussian distribution, suggesting that the Wiener filter may be optimal in this regime.  But the exponential approximation of a Poisson is also very accurate in the fast spiking regime.  To compare these two strategies, we simulated a fast spiking neuron, where the expected number of spikes per bin exceeds 10 (Fig. \ref{fig:FastSpiking}). In this scenario, both the Wiener filter and our fast filter perform approximately equally well.  Both filters infer peaks in the firing rate that are obscured by the low-pass filter properties of the calcium dynamics. Thus, it seems from this analysis that regardless of the firing rate of the observable neuron, (1) filtering the signal may provide valuable information, and (2) our fast filter performs at least as well as the Wiener filter, without requiring more computational time.

\begin{figure}
\centering \includegraphics[width=.9\linewidth]{FastSpiking}
\caption{Comparing the Wiener filter and our fast filter for a fast spiking neuron.  In this scenario, the two filters perform approximately equally well. Note that both recover fast fluctuations in the firing rate that are smoothed out by the calcium dynamics. Conventions as in Fig. \ref{fig:schem}.  Actual and inferred spike trains were normalized similarly, to ease comparisons. Parameters: $\gamma=0.94$, $\nu=0$, $\rho=1$, $\sig=0.3$, $\lam=100$ (modulated by a sinusoid), $\Del=0.05$ msec.} \label{fig:FastSpiking}
\end{figure}


%The Wiener filter differs in construction from our filter in a few ways.  First, it imposes no constraint on on the spike train.  Second, it is optimal upon assuming that the prior distribution of $n_t$ is Gaussian. In our model, spiking was Poisson, \eqref{eq:trans}, which may be well approximated by an exponential in the slow spiking regime, and well approximated by a Gaussian in the fast spiking regime.  and the nonnegativity of $n_t$ in the sparse-spiking regime makes the Gaussian model assumed by the Wiener filter inaccurate (Fig.\ 2, left).  However, when spike rates are fast --- e.g., on average, several spikes per image frame --- a Poisson distribution is better approximated by a Gaussian distribution.  Furthermore, at high firing rates, the mean of the Gaussian would be relatively far from zero, and the variance proportional, so the probability of sampling a negative number would be relatively small, obviating the need for the nonnegativity constraint. Thus, one might expect the Wiener filter to perform as well as our nonnegative filter (and fPPR) when the observed neuron has a high firing rate (and relatively low imaging rate). Furthermore, given that the calcium kernel is exponential, the Wiener filter, like the filters we developed here, only requires $O(T)$ time, as opposed to the typical $O(T\log T)$.

\subsection{Spatial Filtering}

In the above, we assumed a one-dimensional (1-D) observation, $\bF$.  The raw data, however, is actually a series of multidimensional images. To get the 1-D time-series, two pre-processing steps are required.  First, for each neuron, one must define a region-of-interest (ROI), essentially assigning sets of pixels to neurons. This yields a vector time-series for each neuron, $\vbF_t \in \Real^m$.   Second, one must project the $m-$D observations into a 1-D time-series.  Typically, this step is performed by averaging all the pixels within the ROI.  In theory, one can improve on this uniform averaging by estimating the optimal spatial filter. Here, we provide details on how to incorporate this second step (projecting the $m-$D observations into 1-D) into our fast filter. First, we replace Eq \eqref{eq:obj} with:

\begin{align}
\vbF_t &= \ve{\alpha} C_t + \ve{\beta} +  \ve{\Sig} \ve{\varepsilon}_t, \qquad \varepsilon_t \sim \mathcal{N}(\ve{0},\bI) \label{eq:Obs} 
\end{align}

\noindent where $\vbF_t$, $\ve{\alpha}$, and $\beta$ are all $m$ dimensional column vectors, $\ve{\Sig}$ is the covariance matrix, and $\ve{\varepsilon}_t$ is an $m-$dimensional standard normal random vector. $\Alpha$ now represents the optimal spatial filter, and $\Beta$ represents the baseline for each pixel.  Our goal then is to estimate $\Alpha$ and $\Beta$, to provide maximal information about the underlying spike train.  We would expect this optimal spatial filtering improve the effective SNR after filtering in many situations.  For example, if certain pixels are anti-correlated with others, a uniform spatial filter would average out those differences, whereas the optimal filter would take advantage of that information.    

To estimate $\Alpha$ and $\Beta$, we first plug 
%Then, plugging %To estimate the optimal spatial filter, first we assume that both $\bn$ and $\bC$ are known.  Then, letting $\ve{\eta}_v=[\gam \ve{\Alpha}, \nu \Alpha + \Beta, \rho \Alpha]'$, we see that the parameters are again unidentifiable.  This time, we let $\nu=0$ and $\rho=1$, yielding $\ve{\eta}_v=[\gam \ve{\Alpha}, \Beta,\Alpha]'$.  Then, plugging 
Eqs \eqref{eq:Obs} and \eqref{eq:trans} into \eqref{eq:par1}, to obtain:

\begin{align} 
\vec{\bth} &=\argmax_{\vec{\bth} \in \vec{\bTh}} - \frac{T}{2} \log (2\pi|\ve{\Sig}|) + T\log (\lam \Del) -  \lam \Del \bn' \ve{1} \nonumber\\
&-\frac{1}{2} \sum_{t=1}^T (\vbF_t - \ve{\alpha} (\gamma \hC_{t-1} -\nu  - \rho \hn_t) - \ve{\beta})' \ve{\Sig}^{-1} (\vbF_t - \ve{\alpha} (\gamma \hC_{t-1} -\nu  - \rho \hn_t) - \ve{\beta}) %\nonumber \\
\end{align} 

\noindent where $| \cdot |$ indicates the determinant. By pre-whitening the observation matrix, $\vbF=[\vbF_1, \vbF_2, \ldots, \vbF_T]$, we can estimate the covariance matrix by the identity, i.e., $\ve{\Sig}=\ve{I}$.   Without loss of generality, we may assume that $\nu=0$ and $\rho=1$, similar to our assumption of $\alpha=1$ and $\beta=0$ above. This leaves $\gamma$, $\Alpha$, and $\Beta$. Assuming that $\gamma$ is known (or estimated from the raw fluorescence signal, as we did above), we have: 

\begin{align}
\widehat{\ve{\eta}}_v = \argmax_{\norm{\ve{\eta}_v} <1} \norm{\vbF - \ve{\eta}_v \bX_v}^2 \label{eq:Par}
%\ve{\eta}_v &=\argmax_{\ve{\eta}_v, \, 0 < \gamma < 1} %- \frac{T}{2} \log (2\pi|\ve{\Sig}|) + T \log (\lam \Del) -\lam \Del \bn'\ve{1} 
%-(\vbF -\ve{\eta}_v \bX_v) ' \ve{\Sig}^{-1}   (\vbF - \bX \bth_v) \label{eq:Par}
\end{align}

\noindent where $\ve{\eta}_v=[\Alpha, \, \Beta]$, and $\bX_v=[\gamma \bC + \bn, \, \ve{1}]'$. Note that we have constrained the norm of $\ve{\eta}_v$, which is necessary because otherwise there is a free scale between $\bX_v$ and $\ve{\eta}_v$.  Problems of the form as in Eq \eqref{eq:Par} are known as ``trust region subproblems''.  Fortunately, many algorithms for solving such a problem are available \cite{Fortin00}.  


\subsection{Incorporating a nonlinear observation model}

\subsection{Optimal thresholding}

\subsection{Slow dynamics}

\section{Discussion}

\paragraph{Acknowledgments}

Support for JTV was provided by NIDCD DC00109. LP is supported by an NSF CAREER award, by an Alfred P.\ Sloan Research Fellowship, and the McKnight Scholar Award. BOW was supported by NDS grant F30 NS051964. The authors would like to thank A.\ Packer for helpful discussions.

%\subparagraph*{References}
\bibliography{/Users/joshyv/Research/misc/biblist}
\addcontentsline{toc}{section}{References}
%\bibliography{Science}
\bibliographystyle{apalike}
%\bibliographystyle{biophysj}
%\bibliographystyle{nature}


\appendix

\section{Wiener Filter}

Sections \ref{sec:inf} and \ref{sec:est} outline one approach to solving Eq. \eqref{eq:obj}, by approximating the Poisson distribution with an exponential distribution, and imposing a non-negative constraint on the inferred $\hnm$.  Perhaps a more straightforward approach would be to approximate the Poission distribution with a Gaussian distribution.  In fact, as rate increases above about $10$ spikes/sec, a Poisson distribution with rate $\lam \Del$ is well approximated by a Gaussian with mean and variance $\lam \Del$.  Given such an approximation, instead of Eq. \eqref{eq:obj2}, we would obtain:

\begin{align} \label{eq:w1}
\hbn_{w} &\approx \argmin_{n_t \in \Real, \forall t} \sum_{t=1}^T \bigg(\frac{1}{2\sig^2} (F_t-C_t)^2 + \frac{1}{2 \lam \Del} (n_t - \lam \Del)^2\bigg)% \nonumber \\
\end{align}

\noindent As above, we can rewrite Eq. \eqref{eq:w1} in matrix notation in terms of $\bC$:

\begin{align}   \label{eq:w2}
\hbC_{w}&= \argmin_{C_t \in \Real, \forall t} \frac{1}{2\sig^2} \norm{\bF - \bC}^2 + \frac{1}{2\lam\Del} \norm{\bM \bC - \lam\Del\ve{1}}^2 
\end{align}

\noindent which is quadratic in $\bC$, and may therefore be solved analytically using quadratic programming, $\hbC_w = \hbC_0 + \bd_w$, where $\hbC_0$ is the initial guess and $\bd_w=\bH_w \backslash \bg_w$, where

\begin{align}
\bg_w &= \frac{1}{\sig^2} (\bC_0' - \bF) + \frac{1}{\lam\Del} ( (\bM \hbC_0)' \bM - \lam\Del \bM' \ve{1}) \\
\bH_w &= \frac{1}{\sig^2} \ve{I} + \frac{1}{\lam\Del} \bM' \bM
\end{align}

Note that this solution is the optimal linear solution, under the assumption that spikes follow a Gaussian distribution, and if often referred to as the Wiener filter, regression with a smoothing prior, or ridge regression.  To estimate the parameters for the Wiener filter, we take the same approach as above:

\begin{subequations}
\begin{align} \label{eq:w_par}
\hbth_w &\approx \argmax_{\bth_w}P(\ve{F}| \hbn_w, \thet_w) P(\hbn_w | \thet_w) \\
%&= \argmax_{\bth_w} \sum_{t=1}^T \left(-\frac{1}{2} \log (2\pi\sig^2)-\frac{1}{2\sig^2} (F_t - \gamma \hC_{t-1} - \hn_t - \beta)^2 \right) \nonumber \\
%&\qquad \qquad \qquad + \sum_{t=1}^T \bigg(-\frac{1}{2} \log (2\pi\lam\Del)-\frac{1}{2\lam\Del} (\hn_t - \lam\Del)^2 \bigg) \\
&= \argmax_{\bth_w} -\frac{T}{2} \log (4\pi^2\sig^2\lam\Del) - \frac{1}{2\sig^2} \norm{\bY_w + \ve{\eta}_w \bX_w}^2 - \frac{1}{2\lam\Del} \norm{\hbn_w - \lam\Del\ve{1}}^2
\end{align}
\end{subequations}

\noindent where $\bY_w$, $\ve{\eta}_w$, and $\bX_w$ are defined as their subscriptless counterparts in Eq. \eqref{eq:par1}.



\section{old}

%\begin{algorithm}
%\caption{Pseudocode for implementing the optimal nonnegative filter for an exponential prior} \label{alg:bar}
%\begin{algorithmic}
%\WHILE{$\zzz > \epsilon$}
%\STATE Initialize $\zT$
%\STATE $\mathcal{L}_i= \sum_t (\ve{y}_t - \ve{A} \ve{C}_t - \ve{b}) \ve{I}^{-1}  (\ve{y}_t - \ve{A} \ve{C}_t - \ve{b}) +\Del \lam_t n_t - \zzz \log n_t$
%\WHILE{$\mathcal{L}_i < \mathcal{L}_{i-1}$}
%\STATE $\ve{g} = -2  (\ve{y} - \ve{A} \xT - \ve{b}) - \Del \lT' \ve{M} - \zzz \ve{M}' (\zT^{-1})$
%\STATE $\ma{H}= 2 \ve{I} + 2 \zzz \ve{M}' (\zT^{-2}) \ve{M}$
%\STATE Compute $\ve{d}=\ma{H}^{-1}\ve{g}$ efficiently
%\STATE Choose $s$ such that 
%\STATE $\qquad s \geq -\zT(\ve{M} \ve{d})^{-1}$
%\STATE $\qquad$and $\mathcal{L}_{i} < \mathcal{L}_{i-1}$
%\STATE Let $\xT \leftarrow  \xT + s \ve{d}$
%\STATE $i \leftarrow i + 1$
%\ENDWHILE
%\STATE reduce $\zzz$ 
%\ENDWHILE
%\end{algorithmic}
%\end{algorithm}

\begin{figure}
%\centering \includegraphics[width=0.7\linewidth]{NIPSdemo}
\caption{Simulation demonstrating our neuron model and inference. Note that our filter accurately infers the unobserved spike train, given only the noisy fluorescence measurements.  Top panel: Simulated fluorescence. Second panel: Simulated intracellular calcium concentration. Third panel: Simulated spike train.  Fourth panel: Output of optimal nonnegative filter (green) superimposed on simulated spike train (black).  Parameters: $\lam=10$, $\Del=0.025$ msec, $\tau=0.5$ sec, $\sig=1$.} \label{fig:demo}
\end{figure}

\paragraph{Generalization of the optimal nonnegative filter}

The above filter design assumed a very simple model relating spikes to $\Ca$, and $\Ca$ to fluorescence.  Both \eqref{eq:obs} and \eqref{eq:trans} may be generalized in a straightforward manner. In fact, our model may be thought of as a special case of the more general state-space framework:

\begin{subequations} \label{eq:SS}
\begin{align}
\ve{C}_t &= \ve{D} \ve{C}_{t-1} + \ve{1} n_t \label{eq:SSa} \\
\ve{F}_t &= \ve{A} \ve{C}_t + \ve{b} + \ve{\varepsilon}_t \label{eq:SSb}
\end{align}
\end{subequations}

\noindent which we can solve in linear time for any log-concave distribution of $n_t$ and $\ve{\varepsilon}_t$.  For our particular scenario of interest, i.e., that of inferring spike trains from calcium sensitive fluorescence observations, these generalizations facilitate considering a model with much richer dynamics.  For instance, a more accurate model relating spikes to $\Ca$ would consider myriad calcium extrusion mechanisms, buffering, etc. In such a situation, we would represent $\Ca$ as a vector, $\ve{C}_t \in \mathbb{R}^{N \times 1}$, where each element of $\ve{C}_t$ would correspond 
%
%These may all easily be incorporated by replacing the monodimensional $\Ca$ equation, with %:
%
%\begin{align} \label{eq:C}
%$\ve{C}_{t+1} = \ma{D} \ve{C}_t + \ve{1} n_t$ %, \qquad \ve{C}_t \geq 0 \quad \forall t
%\end{align}
%
%\noindent 
%where each element of $\ve{C}_t \in \mathbb{R}^{N \times 1}$ corresponds 
to a different spike dependent calcium process. The differential operator, $\ve{D} \in \mathbb{R}^{N \times N}$, accounts for the relative strength of each of these mechanisms, and their time constants. The spikes, $n_t$, are multiplied by a column vector of ones, $\ve{1} \in \mathbb{R}^{N \times 1}$, because the magnitude of the effect of a spike on each element in $\ve{C}_t$ is scaled by $\ve{\rho}$.\footnote{For the same reasons that there is no parameter scaling the spikes in the monodimensional case} It should be clear that as in \eqref{eq:trans}, spikes are related to calcium by a simple linear transformation.  In the multidimensional scenario, however, $\ve{D}$ --- a matrix --- would replace $a$ --- a scalar ---  so $\ve{M}$ becomes block-bidiagonal, making the Hessian block tridiagonal.  Nonetheless, Gaussian elimination may still solve $\ve{H}\ve{C}=\ve{g}$ in $O(T)$, so our filter may be applied to this scenario as well.

Another natural extension of this work would be to let $F_t$ also be multidimensional.  In \eqref{eq:obs}, although we assumed that we had access to a monodimensional fluorescent magnitude at each time, the raw data is actually a multidimensional movie. These images, however, are necessarily blurred by the point-spread-function of the camera.  Thus, we could replace \eqref{eq:obs} with \eqref{eq:SSb}, where $\ve{A}$ is the point-spread function of the camera, and $\ve{b}$ sets the relative baseline intensity per pixel. %\footnote{In practice, we would concatenate the columns of each image,  making $\ve{F}_t$ a column vector, $\ve{F}_t \in \mathbb{R}^{Q \times 1}$. Furthermore, $\ve{A} \in \mathbb{R}^{Q \times N}$, $\ve{A}$ would be a rank-one matrix, $\ve{A}=\ve{a} \ve{1}'$, where each element of $\ve{a}$ corresponds to the relative weight of each pixel. By parameterizing $\ve{a}$, we could limit the number of parameters for this multidimensional model to be relatively few.} 
Furthermore, the noise, $\ve{\varepsilon}_t$, could have any log-concave distribution, and these results would still hold. Given these generalizations, this filter may be applied to a large class of problems.

%nonnegative deconvolution \cite{SaulLee03} and matrix factorization \cite{LeeSeung99, LeeSeung01} arise in a number of different scenarios, including from audio signals processing \cite{OGradyPearlmutter06} and image processing. We are interested in dealing with a subset of these problems.
%In particular, we assume the existence of a signal of interest, $\zT=n_0,\ldots,n_T$, where each $n_t \in \mathbb{R}_+$ is a scalar nonnegative number,  which is then filtered by a series of linear ordinary differential equations, of the form:
%
%\begin{align} \label{eq:C}
%\ve{C}_{t+1} = \ma{a} (\ve{C}_t + \ve{C}_b) + \ve{1}  n_t %, \qquad \ve{C}_t \geq 0 \quad \forall t
%\end{align}
%
%\noindent where $\ve{C}_t \in \mathbb{R}^{N \times 1}$ is an $N$ dimensional column vector, $\ma{a} \in \mathbb{R}^{N\times N}$ is a differential operator matrix that updates $\ve{C}_t$, $\ve{C}_b$ is the baseline resting value for $\ve{C}_t$, and $\ve{1}$ is a $N$ dimensional column vector of ones.  Observations of this system, $\ve{y}_t \in \mathbb{R}^{M \times 1}$,  are linear-Gaussian functions of $\ve{C}_t$:
%
%\begin{align} \label{eq:y}
%\ve{y}_t = \ve{A} \ve{C}_t + \ve{b} + \ve{\varepsilon}_t, \qquad \ve{\varepsilon}_t \sim \mathcal{N}(0,\ve{I})
%\end{align} 
%
%\noindent where $\ma{A} \in \mathbb{R}^{M\times N}$ is a scaling matrix,  $\ve{b} \in \mathbb{R}^{M \times 1}$ is an offset vector, and  $\varepsilon_t \in \mathbb{R}^{M \times 1}$ is a standard multivariate normal random variable.  Models characterized by \eqref{eq:trans} and \eqref{eq:y} arise in a number of contexts, including several in neuroscience \cite{bj08}.   
%%This signal is then convoled with $J$ exponential filters, each with a unique amplitude and time constant, $A_j$ and $\tau_j$, respectively.  Finally, we make observations, $y_t$ that are corrupted by some Gaussian noise source, $\varepsilon_t$, with mean $\mu$ and variance $\sig^2$, yielding
%%
%%\begin{align}
%%y_t = \sum_{j=1}^J \sum_{t=0}^T C_t \ast A_j e^{-t/\tau_j} + \varepsilon_t, \qquad \varepsilon \sim \mathcal{N}(\mu,\sig^2)
%%\end{align}

\paragraph{Projection Pursuit Regression} 

Projection pursuit regression (PPR) is another technique one could use to infer a spike train from fluorescence measurements.  PPR is different from the optimal nonnegative filter in a few ways.  First, it solves a related, but slightly different problem: instead of finding the MAP estimate of the spike train, PPR finds the maximum likelihood estimate, i.e., the spike train that makes the fluorescence measurements most likely.  Second, PPR constrains the solution to have only nonnegative integers. Therefore, $\zT_{PPR}=\argmax_{n_t \in \mathbb{N}_0} p(\FT | \zT)$, where $\mathbb{N}_0$ is the set of nonnegative integers.. 
% is the solution to the following optimization problem:
%
%\begin{align} \label{eq:ppr}
%\nT_{PPR} = \argmax_{n_t \in \mathbb{n}} p(\FT | \nT)
%\end{align} 
%
Third, because of this constraint, PPR requires an additional parameter, $w$, that sets the size of the calcium transient caused by a single an action potential.  This forces us to substitute \eqref{eq:trans}  with $C_t = a C_{t-1} + w n_t$.  % It may be helpful to think of $\CaT$ as a sum of exponentials, each starting at the time of an action potential.  
%
%More specifically, for this problem, PPR assumes that the signal of interest, $\FT$, is a sum of exponentials times Heaviside step functions and Gaussian noise:
%
%\begin{align}
%\FT = \sum_{t_i} \rho e^{(t_i-t)/\tau} H(t_i-t) + \ve{\varepsilon}
%\end{align}
%
%\noindent where $H(C)=1$ when $C\geq 0$ and zero otherwise, and the goal is to find the spike times, $\{t_i\}$. 
Given these differences, to find $\zT_{PPR}$, PPR proceeds iteratively, adding a spike with each iteration, as long as doing so reduces the residual square error (i.e., the sum of the squared difference between the inferred $\xT$ and $\FT$). One obtains the time of the next inferred spike by finding the maximum of the convolution of the current residual square error with the calcium kernel, $w e^{-t\Del/\tau}$. In general, this procedure takes $O(T \log T)$ per iteration, as the convolution is computed in the Fourier domain.  However, the recursive state-space representation in \eqref{eq:SS} implies that this convolution requires just $O(T)$ time here.  Henceforth, we therefore refer to this approach as fast PPR (or fPPR). This approach may generalized to the multidimensional cases, as in the previous filter. However, unlike the previous filter, the likelihood function is not concave (recall that PPR is a greedy optimization method), so we are no longer guaranteed to converge to the optimal solution. % Nonetheless, this algorithm performs very well for simulated data, as depicted by the bottom panels of Fig. \ref{fig:comp}. 
%Learning the parameters for this model proceeds much like learning those for the optimal nonnegative filter. 

\section{Results} \label{sec:results} %Comparison to other methods}\label{sec:comp}

To evaluate the efficacy of our filters, we compare their results to that of the optimal linear (i.e., Wiener) filter \cite{Wiener49}.  The Wiener filter differs in construction from our filters in a few ways.  First, it imposes no constraint on on the spike train.  Second, it is optimal upon assuming that the prior distribution of $n_t$ is Gaussian. In our model, spiking was Poisson, \eqref{eq:trans}, and the nonnegativity of $n_t$ in the sparse-spiking regime makes the Gaussian model assumed by the Wiener filter inaccurate (Fig.\ 2, left).  However, when spike rates are fast --- e.g., on average, several spikes per image frame --- a Poisson distribution is better approximated by a Gaussian distribution.  Furthermore, at high firing rates, the mean of the Gaussian would be relatively far from zero, and the variance proportional, so the probability of sampling a negative number would be relatively small, obviating the need for the nonnegativity constraint. Thus, one might expect the Wiener filter to perform as well as our nonnegative filter (and fPPR) when the observed neuron has a high firing rate (and relatively low imaging rate). Furthermore, given that the calcium kernel is exponential, the Wiener filter, like the filters we developed here, only requires $O(T)$ time, as opposed to the typical $O(T\log T)$.

Fig.\ \ref{fig:comp} depicts a comparison between the filters developed above and the Wiener filter for two different scenarios: a slow firing rate simulation (left panels) and a fast firing rate simulation (right panels). When action potentials are sparse, the two filters we propose above outperform the Wiener filter. Moreover, at high firing rates,  all three filters perform approximately equally well. 

%Perhaps the most closely related filter to the filters developed above is the Wiener filter \cite{??}.   Fig. \ref{fig:comp} shows two example fluorescent traces.  One the left, a neuron was simulated with a rate of $1$ Hz; on the right, $10$ Hz. The top and second panels show the simulated fluorescence and spike train, respectively.  The third panels show the performance of the optimal linear (i.e., Wiener) filter; whereas the fourth panels show the performance of the optimal nonnegative filter. Both filters perform very well in the scenario when there are on average several spikes per frame.   

%\begin{minipage}{1.0\textwidth}
\begin{figure}
%\includegraphics[width=1.0\linewidth]{NIPScomp}
\caption{Comparison of various linear filters for a simulated Poisson neuron spiking with a rate of $1$ Hz (left panels) and $200$ Hz (right panels). The main result is that the two filters proposed outperform the optimal linear (i.e., Wiener) filter. Top panels: Fluorescence measurements.  Second panels: Number of spikes per frame.  Third panels: Optimal linear (i.e., Wiener) filter output given the above fluorescence signal.  The Wiener filter does not impose a nonnegativity constraint, thus the inferred spike train can either be positive (green) or negative (red).  Fourth panels: Same as third panels, but for the optimal nonnegative filter. Note the absence of negative spikes.  Fifth panels: Same as third panels, but using the fast Projection Pursuit Regression (fPPR) filter. Parameters: $\Del=0.025$ sec, $\tau=0.5$ sec, $\lam=1/($firing rate$)$, left  $\sig=0.4$, right $\sig=1$.} \label{fig:comp}
\end{figure}

\begin{figure}[!h]
%\centering \includegraphics[width=.6\linewidth]{NIPSdata}
\caption{Inferring a spike train given fluorescence measurements from a live \emph{in vitro} neuron.  Simultaneous recording of a saturating fluorescence signal (black line in top panel), and its associated spike train (black impluses in all panels).  Note how the three filters handle saturation differently. The optimal nonnegative filter's signal-to-noise ratio degrades as saturation increases, but does not suffer a catastrophic failure (green in second panel). fPPR suffers more than the optimal nonlinear particle filter when the signal saturates, due to the hard threshold (green in third panel). The optimal nonlinear particle filter \cite{BJ08}, which explicitly incorporates saturation, correctly identifies each spike time (green in bottom panel).} \label{fig:real}
\end{figure}
%\end{minipage}

While in simulations, all the above algorithms perform reasonably well, the true test is how well they perform given data from live cells. We simultaneously recorded both electrophysiologically and imaged with an epifluorescent microscope, from a pyramidal neuron in a somatosensory cortical slice, as described in \cite{MacLeanYuste05}. Fig.\ \ref{fig:real} shows an example fluorescence time-series in which the neuron spiked with a relatively low rate, but the calcium accumulated nonetheless, leading to fluorescence saturation (top panel). In practice, this kind of strong saturation is rarely observed (personal communication, anonymous% R.\ Friedrich
), so this example is designed to test the limits of performance of our filters. %As all the above described methods are inherently linear methods, their assumptions do not adequately capture these nonlinear dynamics.
In fact, the optimal nonnegative filter accurately infers every spike, even when the fluorescence is strongly saturating, and the signal-to-noise ratio is very poor (second panel). On the other hand, fPPR, which has a sharp threshold for including an additional spike, performs relatively poorly (third panel), demonstrating the dependency of this method on a good model fit.  For comparison purposes, we also show the performance of an optimal nonlinear particle filter \cite{BJ08}, which specifically incorporates a nonlinear saturation function. While the optimal nonlinear particle filter performs better in scenarios such as this one, the computational burden is increased by approximately $100$-fold relative to the fast nonnegative filter. Thus these two filters serve complementary purposes: the fast nonnegative filter is better geared to rapid online analysis of large-scale multi-neuronal data, whereas the nonlinear particle filter is better suited for offline refinement of the results from the fast nonnegative filter.   

\section{Conclusions} \label{sec:dis}

We show here that for certain nonnegative deconvolution problems, we can derive an algorithm that is both optimal and efficient.  More specifically, our algorithm may be applied to any model with a nonnegative signal that is linearly filtered by a matrix linear ordinary differential equation.  We apply this approach to the problem of inferring the most likely spike train given noisy calcium sensitive fluorescence observations (c.f. Fig.\ \ref{fig:demo}), and demonstrate, in simulations, that the optimal nonnegative filter outperforms the optimal linear (i.e., Wiener) filter in both slow and fast firing rate regimes (c.f. Fig.\ \ref{fig:comp}).  Furthermore, when applied to data from a live cell, the optimal nonnegative filter outperforms a fast projection pursuit regression filter, which constrains the inferred spike train to be nonnegative integers (c.f. Fig.\ \ref{fig:real}). On the other hand, the nonnegative filter is based on a linear observation model, and therefore suffers a loss of precision in the presence of strong saturation effects, in contrast to the optimal nonlinear particle filter (c.f. Fig.\ \ref{fig:real}).    

The implications of these results are severalfold.  First, it seems as if there is no reason to use the Wiener filter for scenarios in which our algorithm may apply.  Second, as our filter is so efficient, it may be used for many real-time processing applications.  Specifically, upon simultaneously imaging a population of neurons \cite{IkegayaYuste04, NiellSmith05, OhkiReid05, YaksiFriedrich06, SatoSvoboda07}, our filter may be applied essentially online.  This could greatly expedite the tuning of important experimental parameters --- such as laser intensity --- to optimize signal-to-noise ratio for inferring spikes.  Third, the parameters estimated from this filter may be used to initialize the parameters of the optimal nonlinear particle filter, which may then be used offline, to further refine the spike train inference. % Because the optimal nonlinear particle filter performs in approximately real-time (making it $\sim 100$ fold slower than the filters developed here), it may be run overnight on all the neural data collected in a daily experimental session. 
Future work will consider multidimensional models for this application, incorporating both more sophisticated calcium models, and spatial filtering for extracting the fluorescence signal, obviating the need for additional algorithms for image segmentation. 

\section{other}

We have found empirically that despite the approximation in \eqref{eq:obj2}, upon initializing the parameters with a reasonable guess, the algorithm tends to converge to parameters that are reasonably close to the true parameters, resulting in an accurate spike reconstruction. Importantly, estimating these parameters typically requires only a very short sequence of observations, e.g., several seconds of data including about $5$--$10$ spikes (not shown).


\end{document}
