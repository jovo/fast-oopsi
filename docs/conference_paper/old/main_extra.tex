There are a number of ways one could go about inferring the hidden spike train from the fluorescence time series.  Below, we develop a number of Bayesian approaches, in which we first define a likelihood that we would like to maximize, and then propose an efficient algorithm for maximizing that likelihood.

\paragraph{Least-Squares Solution}

We would like to find the most likely spike train that led to the observed fluorescence signal. This problem may be formally cast as finding a spike train that maximizes the likelihood of the fluorescence observations.  As the only source of noise is the $\varepsilon_t$ on the observations, which is Gaussian, we have: 

\begin{align} \label{eq:lsq}
\ve{n}_{lsq} &= \argmax_{\ve{n}} \sum_{t=0}^T \varepsilon_t^2\\
&= \argmax_{\ve{n}} \sum_{t=0}^T \frac{1}{2\sigma^2}\exp\left\{-\frac{1}{2}\left(\frac{F_t-\Ca_t}{\sigma}\right)^2\right\}\\
&= \argmin_{\ve{n}} \sum_{t=0}^T (F_t - \Ca_t)^2
\end{align}

\noindent where we use the notation $\ve{n}=n_0,\ldots, n_T$, and $\Ca$ is implicitly a function of $n$.  As \eqref{eq:lsq} is a quadratic optimization problem, it may be solved with any gradient ascent technique.  In particular, for any quadratic problem:

\begin{align} \label{eq:lsq0}
\ve{x}_{lsq} = \argmin_{\ve{x}} \norm{\ma{Y} \ve{x} - \ve{z}}_2^2,
\end{align}

\noindent the analytical solution is simply:

\begin{align}
\ve{x}_{lsq} = \ve{x}_0 - (\ma{Y}'\ma{Y})^{-1} \ma{Y}'\ve{z} 
= \ve{x}_0-\ma{H}^{-1}\ve{g}
\end{align}

\noindent where $\ve{x}_0$ is some initial guess for the value of $\ve{x}$, $\ma{H}$ is the Hessian matrix (i.e., second derivative of $\norm{\ma{Y} \ve{x} + \ve{z}}_2^2$ with respect to $\ve{x}$, and $\ve{g}$ is the gradient vector (i.e., first  derivative of $\norm{\ma{Y} \ve{x} + \ve{z}}_2^2$ with respect to $\ve{x}$. Thus to solve any quadratic optimization problem, one must simply cast it in the form of \eqref{eq:lsq0} and then solve for $\ma{H}$ and $\ve{h}$. The key to solving \eqref{eq:lsq} problem efficiently is to write $\ve{n}$ as a function of $\Cav=[\Ca_0,\ldots,\Ca_T]$. In particular, after subtracting $b$, we may write:

\begin{align} \label{eq:nMC}
\ve{n}=\ma{M}\Cav
\end{align}

\noindent where $\ma{M}$ is a $T \times T$ bidiagonal deconvolution matrix:

\begin{equation} \label{eq:M}
\ma{M}=\begin{bmatrix}
1/A&0&0&0&\ldots&0\\
-a/A&1/A&0&0&\ldots&0\\
0&-a/A&1/A&0&\ldots&0\\
\vdots\\
0&\ldots&0&0&-a/A&1/A
\end{bmatrix}.
\end{equation}

\noindent which follows from \eqref{eq:C}. Thus, letting $c=1/(2\sigma^2)$ we may rewrite \eqref{eq:lsq}, and solve for $\ve{g}$ and $\ma{H}$:

\begin{align}
\Cav_{lsq} &= \argmin_{\Cav} c\norm{\ve{F}-\Cav}_2^2\\
\ve{g} &= -2c(\ve{F}-\Cav)\\
\ma{H}&= 2c\ma{I}
\end{align}

\noindent where $\ma{I}$ is an identity matrix of appropriate size.  Thus,

\begin{align}
\Cav_{lsq} = \Cav-(2\ma{I})^{-1} \big(-2c(\ve{F}-\Cav)\big)=\ve{F},
\end{align}

\noindent and therefore, $\ve{n}_{lsq}=\ma{M}\ve{F}$.
%Because the likelihood $\Lik$ of $\ve{n}_{\eta}$ is log-concave, we can use the Newton approach, which requires computing the gradient $\ve{g}$ and Hessian $\ma{H}$, and stepping in the direction of $\ma{H}^{-1}\ve{g}$.  
%Because $\ma{M}$ is bidiagonal, $\ma{H}$ is tridiagonal, so this computation may be performed in $O(T)$ time instead of the typical $O(T^3)$ time required for matrix inversions. 
%(details for efficiently computing the necessary terms may be found in Appendix \ref{sec:bar}).  Figure \ref{fig:comp}(D) shows why the solution to \eqref{eq:lsq} is problematic. 
This solution, plotted in Figure \ref{fig:comp}(D), is problematic for several reasons. First, in periods of quiescence (i.e., no spiking), the algorithm incorrectly infers the presence of many ``spikelets'', which do not exist in the true data.  Second, the algorithm sometimes infers \emph{negative} spikes, which also cannot exist. Third, the number of spikes in a frame is not necessarily an integer.  Each of these issues may be treated using standard tools from the optimization literature.

\paragraph{Regularizing}

To deal with the spikelets, we use a technique called ``regularizing''.  Essentially, we impose a prior on the probability of their being a spike at any time, and then we penalize the solution if it infers too many spikes.  Formally, we write:

\begin{align} \label{eq:reg}
\ve{n}_{reg} &= \argmin_{\ve{n}} \sum_{t=0}^T c \big((F_t - \Ca_t)^2 + f_{reg}(n_t) \big),
\end{align}

\noindent where $f_{reg}(n_t)$ is some regularizing function of $n_t$, typically chosen such that the likelihood remains concave.  For example, $f_{reg}(n_t)=\lambda_t n_t^2$, where $\lambda_t=1/($expected $n_t$), then we have a standard \emph{smoothing prior}.\footnote{This may be thought of as a generalization of ridge regression and Wiener deconvolution.}  To solve \eqref{eq:reg} efficiently with a smoothing prior, we rewrite \eqref{eq:reg} as a function of $\Cav$:

\begin{align} \label{eq:reg2}
\Cav_{reg} &= \argmin_{\Cav} c\norm{\ve{F}-\Cav}_2^2 + \norm{\sml\ma{M}\Cav}_2^2
\end{align}


\noindent where $\sml$ is a matrix with components $\sqrt{\lambda_t}$ along the diagonal and zeros elsewhere. Because this is also a quadratic problem, the solution is given by finding Hessian and gradient with respect to $\Cav$:

\begin{align}
\ve{g} &= -2c(\ve{F}-\Cav)+2 (\sml\ma{M}\Cav)'(\sml\ma{M})\\
\ma{H} &= 2c\ma{I}+2(\sml\ma{M})'(\sml\ma{M})
\end{align}

The key to solving this problem efficiently is that the Hessian is a tridiagonal matrix (which follows from the fact that $\sml$ is diagonal and $\ma{M}$ is bidiagonal).  Therefore, $\ma{H}^{-1}\ve{g}$ may be solved in $O(T)$ time (for instance, using Matlab's $\backslash$), instead of the typical $O(T^3)$ time normally required.  Figure \ref{fig:comp}(F) shows the results of \eqref{eq:reg2}.  Problematically, the solutions from such an approach possibly yield negative spikes, which are not possible in the true spike train.  

\section{Wiener Deconvolution} \label{sec:wiener}

We can write the above model as a convolution:

\begin{align}
F(t) = y(t) \ast n(t) + \varepsilon(t)
\end{align}

\noindent where $y(t)=Ae^{-t/\tau}$ is the calcium convolution kernel. The goal is then to find some optimal deconvolution kernel, $g(t)$, so that we may estimate $n_t$ as follows:

\begin{align} \label{eq:opt_n}
\widehat{n}(t) = g(t) \ast F(t).
\end{align}

The Wiener deconvolution filter is most easily described in the frequency domain:

\begin{align} \label{eq:G}
G(f) = \frac{|Y^(f)| E[N(f)]^2}{|Y(f)|^2 E[N(f)]^2 + E[\epsilon(f)]^2}
\end{align}

\noindent where we use the notation $X(f)$ to indicate the Fourier transform of $x(t)$, and $E[X(f)]^2$ to indicate the power spectral density (PSD) of $x(t)$.  By assuming that $n(t)$ is Poisson (or binomial), and $\varepsilon(t)$ is Gaussian, we can write their PSD's analytically:

\begin{align} \label{eq:psdN}
E[N(f)]^2 &=\\ \label{eq:psde}
E[\epsilon(f)]^2&=.
\end{align}

Similarly, because $y(t)$ is an exponential, we may write its Fourier transform as:

\begin{align} \label{eq:Y}
Y(f)=.
\end{align}

Thus, plugging in \eqref{eq:psdN},\eqref{eq:psde}, and \eqref{eq:Y} into \eqref{eq:G}, we have

\begin{align}
G(f)&=.
\end{align}

Finally, taking the inverse Fourier transform of $G(f)$, we are left with

\begin{align}
g(t)=,
\end{align}

\noindent which we may plug back into \eqref{eq:opt_n} to find $\widehat{n}(t)$.


\subsection{Lin Alg way}

Weiner deconv of exponential kernel.

First note that the $\Ca_t$ is simply a convolution of the spike train with a linear kernel,

\begin{align}
\ve{\Ca} = Ae^{-\ve{t}/\tau_c} \ast \ve{n}
\end{align}

\noindent where $\ve{t}$ indicates the vector, $[A, Ae^{-\Delta/\tau_c},\ldots, Ae^{-L\Delta/\tau_c}]$, with $L$ the length of this linear kernel.  The spike train may also be written as a linear function of $\Ca_t$:


\begin{align}
\ve{n} = \ma{Y} \ve{\psi} + \ve{\varepsilon}
\end{align}

\noindent where $\ma{Y}$ is a $T \times L$ matrix

\begin{equation}
\begin{bmatrix}
\Ca_0&\Ca_1&\ldots&\Ca_{L-1}&1\\
\Ca_1&\Ca_2&\ldots&\Ca_L&1\\
\vdots\\
\Ca_{T-L}&\Ca_{T-L+1}&\ldots&\Ca_T&1
\end{bmatrix}.
\end{equation}

The solution for the optimal $\ve{\psi}$ is thus given by,

\begin{align}
E[\ve{\psi}] &= E[(\ma{Y}^T\ma{Y})^{-1}(\ma{Y}^T\ve{n})]\\
&=E[\ma{Y}^T\ma{Y}]^{-1}E[\ma{Y}^T\ve{n}]
\end{align}

Importantly, $\ma{Y}$ may be written as a linear function of $\ve{n}$:

\begin{align} \label{eq:YKn}
\ma{Y}&=\ma{K}\ve{n}+\ve{\varepsilon}\\
\Rightarrow E[\ma{Y}] &= \ma{K}E[\ve{n}]
\end{align}

\noindent where $\ma{K}$ is the convolution kernel matrix:

\begin{equation}
\begin{bmatrix}
A&0&\ldots&0\\
Ae^{-\Delta/\tau_c}&A&0&\ldots\\
Ae^{-2\Delta/\tau_c}&Ae^{-\Delta/\tau_c}&A&\ldots\\
\vdots\\
Ae^{-T\Delta/\tau_c}&\ldots&\ldots&A
\end{bmatrix}.
\end{equation}

Thus, \eqref{eq:YKn} may be expanded

\begin{align}
E[\ma{Y}^T\ma{Y}]^{-1}E[\ma{Y}^T\ve{n}] &= (\ma{K}E[\ve{n}])^T(\ma{K}E[\ve{n})]^{-1} (\ma{K}E[\ve{n}])^T E[\ve{n}]\\
&=\big((E[\ve{n}]^T\ma{K}^T)(\ma{K}E[\ve{n}])\big)^{-1}E[\ve{n}^T]\ma{K}^TE[\ve{n}].
\end{align}

Converting to the Fourier domain, we have

\begin{align}
\ma{K} &= \frac{1}{1+a\omega}\\
\ma{K}^T\ma{K} &= \frac{1}{1+a\omega^2}\\
\ma{K}^T\ma{K}+b\ma{I} &= b+\frac{1}{1+a\omega^2}\\
E[\ve{n}] &= \lambda
\end{align}


\section{Efficient Implementation of the Barrier Method} \label{sec:bar}

We are trying to solve \eqref{eq:b} for a particular value of $\eta$.  For each value of $\eta$, we can efficiently solve this problem using the Newton's method, as it is concave. First, we must compute the gradient (first derivative) and Hessian (second derivative) of the objective in \eqref{eq:b} with respect to $\Ca$, which requires that we write $n_t$ as a function of $\Ca_t$, which we can using \eqref{eq:nMC}. Then, we use these derivatives to choose the direction to ascend. The likelihood, gradient, and Hessian are, in matrix notation:

\begin{align}
\Lik &= \norm{\ve{F}-\Cav}^2_2 + \lambda \ve{1}' \ve{n} - \eta \log \ve{n}\\
&= \norm{\ve{F}-\Cav}^2_2 + \lambda \ve{1}' \ma{M} \Cav - \eta \log \ma{M} \Cav\\
\ve{g} &= -2(\ve{F}-\Cav) +\lambda \ve{1}' \ma{M} - \eta \ma{M}' \log (\ve{n}^{-1}) \label{eq:g}\\
\ma{H} &= 2\ma{I} + 2 \eta \ma{M}' \log(\ve{n}^{-2}) \ma{M} \label{eq:H}
\end{align}

\noindent where $\ma{I}$ is the identity matrix of appropriate size.  Now the key is that the Hessian is a tridiagonal matrix, which follows from the fact that $\ma{M}$ is bidiagonal.  Therefore, to compute the typical Newton step, update $\Cav$ as follows:

\begin{align}
\Cav \leftarrow \Cav + s \ve{d} 
\end{align}

\noindent where $\ve{d}=\ma{H}^{-1} \ve{g}$, which may be performed in $O(T)$ time using standard efficient algorithms (e.g., Matlab's $\backslash$), as opposed to the more general $O(T^3)$ time, due to the tridiagonal nature of $\ma{H}$.  The step size $s$ is typically taken to be unity, unless doing so violates any of the constraints.  For this problem, we require that $n_t>0$ for all $t$; therefore, we have:

\begin{align}
0&< n_t\\
&< \ma{M}(\ve{C} + s \ve{d})\\
&< \ma{M} \ve{C} + s \ma{M} \ve{d}\\
\Rightarrow s &> -\ma{M}\ve{C} (\ma{M} \ve{d})^{-1}.
\end{align}

\noindent So $s$ must be larger than every component of the vector on the right-hand-side of the last inequality.  Furthermore, it is possible to ''over-step'', or rather, take a step so large as to \emph{increase} the likelihood that we are trying to minimize (we climbed over the hill and started going back down again).  As such, prior to stepping, we check that the likelihood would indeed decrease, if not, we reduce $s$ until it does.  Pseudocode for an efficient implementation of this approach is provided in Table \ref{tab:bar}.  Note that to solve \eqref{eq:lsq}, $\ve{g}$ is simply $2(\Cav-\ve{F})$ and $\ma{H}$ is 2, and because the likelihood is quadratic, stepping once to $\Cav+\ma{H}^{-1}\ve{g}$ yields $\ve{n}_{lsq}$.


\begin{table}[h]
\caption{Pseudocode for barrier method}
\label{tab:bar}
\fbox{\begin{minipage}{1.0\linewidth}
For each $\eta$, let $\Cav_i$ be the current estimate of $\Cav$ (which may be initialized at $\ve{1}$ for instance).  While $\Cav_i < \Cav_{i-1}+\xi$ (for some small $\xi$), iterate the following steps.  When complete, reduce $\eta$ and repeat.
\begin{itemize}
\item Compute $\ve{g}$ using \eqref{eq:g}
\item Compute $\ma{H}$ using \eqref{eq:H}
\item Compute $\ve{d}=\ma{H}^{-1}\ve{g}$ efficiently
\item Let $s=\min(1,0.99 \min(-\ma{M}\ve{C} (\ma{M}\ve{d})^{-1})$
\item Let $\Cav_{temp} = \Cav_i + s \ve{d}$
\item If $\mathcal{L}(\Cav_{temp}) > \mathcal{L}(\Cav_i)$, then reduce $s$ until the opposite it true. Else, $\Cav_{i+1}=\Cav_{temp}$.
\end{itemize}
\end{minipage}}
\end{table}

\section{Efficient Implementation of the Barrier Method} \label{sec:bar}

We are trying to solve \eqref{eq:b} for a particular value of $\eta$.  For each value of $\eta$, we can efficiently solve this problem using the Newton's method, as it is concave. First, we must compute the gradient (first derivative) and Hessian (second derivative) of the objective in \eqref{eq:b} with respect to $\Ca$, which requires that we write $n_t$ as a function of $\Ca_t$, which we can using \eqref{eq:nMC}. Then, we use these derivatives to choose the direction to ascend. The likelihood, gradient, and Hessian are, in matrix notation:

\begin{align}
\Lik &= \norm{\ve{F}-\Cav}^2_2 + \lambda \ve{1}' \ve{n} - \eta \log \ve{n}\\
&= \norm{\ve{F}-\Cav}^2_2 + \lambda \ve{1}' \ma{M} \Cav - \eta \log \ma{M} \Cav\\
\ve{g} &= -2(\ve{F}-\Cav) +\lambda \ve{1}' \ma{M} - \eta \ma{M}' \log (\ve{n}^{-1}) \label{eq:g}\\
\ma{H} &= 2\ma{I} + 2 \eta \ma{M}' \log(\ve{n}^{-2}) \ma{M} \label{eq:H}
\end{align}

\noindent where $\ma{I}$ is the identity matrix of appropriate size.  Now the key is that the Hessian is a tridiagonal matrix, which follows from the fact that $\ma{M}$ is bidiagonal.  Therefore, to compute the typical Newton step, update $\Cav$ as follows:

\begin{align}
\Cav \leftarrow \Cav + s \ve{d} 
\end{align}

\noindent where $\ve{d}=\ma{H}^{-1} \ve{g}$, which may be performed in $O(T)$ time using standard efficient algorithms (e.g., Matlab's $\backslash$), as opposed to the more general $O(T^3)$ time, due to the tridiagonal nature of $\ma{H}$.  The step size $s$ is typically taken to be unity, unless doing so violates any of the constraints.  For this problem, we require that $n_t>0$ for all $t$; therefore, we have:

\begin{table}[h]
\caption{Pseudocode for PPR}
\label{tab:bar}
\fbox{\begin{minipage}{1.0\linewidth}
This algorithm iterates several steps, as schematized by Figure \ref{fig:ppr_schem}. The algorithm is initialized with the fluorescence signal, which is the residual before iterating the following steps, $\ve{r}_0$:
\begin{itemize}
\item Compute the cross-correlation between the residual and the calcium kernel, 
$ \ve{r}_0 \otimes A e^{-t/\tau}. $
\item Let $t_{i+1}$ be the maximum of that cross-correlation,
$ t_{i+1} = \max_t \ve{r}_0 \otimes A e^{-t/\tau}. $
\item Convolve the calcium kernel with $t_i$ to generate the expected effect of a spike having occurred at that time step,
$ t_{i+1} \ast e^{-t/\tau}. $
\item Subtract the result of \eqref{eq:t_i} from the residual of the previous step to get the new residual,
$ r_{i+1} = r_i -  t_{i+1} \ast e^{-t/\tau} $
\item Compute the regularized sum of residual error, where $n_{i+1,t}$ is a vector of zeros with ones for each $t=t_i$,
$ \epsilon_{i+1} = \sum_t r_{i+1,t} + \lambda_t n_{i+1,t} $
\item If $r_{i+1} < r_i$, repeat.  Otherwise, let $\ve{n}_{ppr} = \ve{n}_i$.
\end{itemize}
\end{minipage}}
\end{table}

