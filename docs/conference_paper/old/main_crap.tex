\section{Wiener Deconvolution} \label{sec:wiener}

We can write the above model as a convolution:

\begin{align}
F(t) = y(t) \ast n(t) + \varepsilon(t)
\end{align}

\noindent where $y(t)=Ae^{-t/\tau}$ is the calcium convolution kernel. The goal is then to find some optimal deconvolution kernel, $g(t)$, so that we may estimate $n_t$ as follows:

\begin{align} \label{eq:opt_n}
\widehat{n}(t) = g(t) \ast F(t).
\end{align}

The Wiener deconvolution filter is most easily described in the frequency domain:

\begin{align} \label{eq:G}
G(f) = \frac{|Y^(f)| E[N(f)]^2}{|Y(f)|^2 E[N(f)]^2 + E[\epsilon(f)]^2}
\end{align}

\noindent where we use the notation $X(f)$ to indicate the Fourier transform of $x(t)$, and $E[X(f)]^2$ to indicate the power spectral density (PSD) of $x(t)$.  By assuming that $n(t)$ is Poisson (or binomial), and $\varepsilon(t)$ is Gaussian, we can write their PSD's analytically:

\begin{align} \label{eq:psdN}
E[N(f)]^2 &=\\ \label{eq:psde}
E[\epsilon(f)]^2&=.
\end{align}

Similarly, because $y(t)$ is an exponential, we may write its Fourier transform as:

\begin{align} \label{eq:Y}
Y(f)=.
\end{align}

Thus, plugging in \eqref{eq:psdN},\eqref{eq:psde}, and \eqref{eq:Y} into \eqref{eq:G}, we have

\begin{align}
G(f)&=.
\end{align}

Finally, taking the inverse Fourier transform of $G(f)$, we are left with

\begin{align}
g(t)=,
\end{align}

\noindent which we may plug back into \eqref{eq:opt_n} to find $\widehat{n}(t)$.


\subsection{Lin Alg way}

Weiner deconv of exponential kernel.

First note that the $\Ca_t$ is simply a convolution of the spike train with a linear kernel,

\begin{align}
\ve{\Ca} = Ae^{-\ve{t}/\tau_c} \ast \ve{n}
\end{align}

\noindent where $\ve{t}$ indicates the vector, $[A, Ae^{-\Delta/\tau_c},\ldots, Ae^{-L\Delta/\tau_c}]$, with $L$ the length of this linear kernel.  The spike train may also be written as a linear function of $\Ca_t$:


\begin{align}
\ve{n} = \ma{Y} \ve{\psi} + \ve{\varepsilon}
\end{align}

\noindent where $\ma{Y}$ is a $T \times L$ matrix

\begin{equation}
\begin{bmatrix}
\Ca_0&\Ca_1&\ldots&\Ca_{L-1}&1\\
\Ca_1&\Ca_2&\ldots&\Ca_L&1\\
\vdots\\
\Ca_{T-L}&\Ca_{T-L+1}&\ldots&\Ca_T&1
\end{bmatrix}.
\end{equation}

The solution for the optimal $\ve{\psi}$ is thus given by,

\begin{align}
E[\ve{\psi}] &= E[(\ma{Y}^T\ma{Y})^{-1}(\ma{Y}^T\ve{n})]\\
&=E[\ma{Y}^T\ma{Y}]^{-1}E[\ma{Y}^T\ve{n}]
\end{align}

Importantly, $\ma{Y}$ may be written as a linear function of $\ve{n}$:

\begin{align} \label{eq:YKn}
\ma{Y}&=\ma{K}\ve{n}+\ve{\varepsilon}\\
\Rightarrow E[\ma{Y}] &= \ma{K}E[\ve{n}]
\end{align}

\noindent where $\ma{K}$ is the convolution kernel matrix:

\begin{equation}
\begin{bmatrix}
A&0&\ldots&0\\
Ae^{-\Delta/\tau_c}&A&0&\ldots\\
Ae^{-2\Delta/\tau_c}&Ae^{-\Delta/\tau_c}&A&\ldots\\
\vdots\\
Ae^{-T\Delta/\tau_c}&\ldots&\ldots&A
\end{bmatrix}.
\end{equation}

Thus, \eqref{eq:YKn} may be expanded

\begin{align}
E[\ma{Y}^T\ma{Y}]^{-1}E[\ma{Y}^T\ve{n}] &= (\ma{K}E[\ve{n}])^T(\ma{K}E[\ve{n})]^{-1} (\ma{K}E[\ve{n}])^T E[\ve{n}]\\
&=\big((E[\ve{n}]^T\ma{K}^T)(\ma{K}E[\ve{n}])\big)^{-1}E[\ve{n}^T]\ma{K}^TE[\ve{n}].
\end{align}

Converting to the Fourier domain, we have

\begin{align}
\ma{K} &= \frac{1}{1+a\omega}\\
\ma{K}^T\ma{K} &= \frac{1}{1+a\omega^2}\\
\ma{K}^T\ma{K}+b\ma{I} &= b+\frac{1}{1+a\omega^2}\\
E[\ve{n}] &= \lambda
\end{align}


\section{Efficient Implementation of the Barrier Method} \label{sec:bar}

We are trying to solve \eqref{eq:b} for a particular value of $\eta$.  For each value of $\eta$, we can efficiently solve this problem using the Newton's method, as it is concave. First, we must compute the gradient (first derivative) and Hessian (second derivative) of the objective in \eqref{eq:b} with respect to $\Ca$, which requires that we write $n_t$ as a function of $\Ca_t$, which we can using \eqref{eq:nMC}. Then, we use these derivatives to choose the direction to ascend. The likelihood, gradient, and Hessian are, in matrix notation:

\begin{align}
\Lik &= \norm{\ve{F}-\Cav}^2_2 + \lambda \ve{1}' \ve{n} - \eta \log \ve{n}\\
&= \norm{\ve{F}-\Cav}^2_2 + \lambda \ve{1}' \ma{M} \Cav - \eta \log \ma{M} \Cav\\
\ve{g} &= -2(\ve{F}-\Cav) +\lambda \ve{1}' \ma{M} - \eta \ma{M}' \log (\ve{n}^{-1}) \label{eq:g}\\
\ma{H} &= 2\ma{I} + 2 \eta \ma{M}' \log(\ve{n}^{-2}) \ma{M} \label{eq:H}
\end{align}

\noindent where $\ma{I}$ is the identity matrix of appropriate size.  Now the key is that the Hessian is a tridiagonal matrix, which follows from the fact that $\ma{M}$ is bidiagonal.  Therefore, to compute the typical Newton step, update $\Cav$ as follows:

\begin{align}
\Cav \leftarrow \Cav + s \ve{d} 
\end{align}

\noindent where $\ve{d}=\ma{H}^{-1} \ve{g}$, which may be performed in $O(T)$ time using standard efficient algorithms (e.g., Matlab's $\backslash$), as opposed to the more general $O(T^3)$ time, due to the tridiagonal nature of $\ma{H}$.  The step size $s$ is typically taken to be unity, unless doing so violates any of the constraints.  For this problem, we require that $n_t>0$ for all $t$; therefore, we have:

\begin{align}
0&< n_t\\
&< \ma{M}(\ve{C} + s \ve{d})\\
&< \ma{M} \ve{C} + s \ma{M} \ve{d}\\
\Rightarrow s &> -\ma{M}\ve{C} (\ma{M} \ve{d})^{-1}.
\end{align}

\noindent So $s$ must be larger than every component of the vector on the right-hand-side of the last inequality.  Furthermore, it is possible to ''over-step'', or rather, take a step so large as to \emph{increase} the likelihood that we are trying to minimize (we climbed over the hill and started going back down again).  As such, prior to stepping, we check that the likelihood would indeed decrease, if not, we reduce $s$ until it does.  Pseudocode for an efficient implementation of this approach is provided in Table \ref{tab:bar}.  Note that to solve \eqref{eq:lsq}, $\ve{g}$ is simply $2(\Cav-\ve{F})$ and $\ma{H}$ is 2, and because the likelihood is quadratic, stepping once to $\Cav+\ma{H}^{-1}\ve{g}$ yields $\ve{n}_{lsq}$.


\begin{table}[h]
\caption{Pseudocode for barrier method}
\label{tab:bar}
\fbox{\begin{minipage}{1.0\linewidth}
For each $\eta$, let $\Cav_i$ be the current estimate of $\Cav$ (which may be initialized at $\ve{1}$ for instance).  While $\Cav_i < \Cav_{i-1}+\xi$ (for some small $\xi$), iterate the following steps.  When complete, reduce $\eta$ and repeat.
\begin{itemize}
\item Compute $\ve{g}$ using \eqref{eq:g}
\item Compute $\ma{H}$ using \eqref{eq:H}
\item Compute $\ve{d}=\ma{H}^{-1}\ve{g}$ efficiently
\item Let $s=\min(1,0.99 \min(-\ma{M}\ve{C} (\ma{M}\ve{d})^{-1})$
\item Let $\Cav_{temp} = \Cav_i + s \ve{d}$
\item If $\mathcal{L}(\Cav_{temp}) > \mathcal{L}(\Cav_i)$, then reduce $s$ until the opposite it true. Else, $\Cav_{i+1}=\Cav_{temp}$.
\end{itemize}
\end{minipage}}
\end{table}

\section{Efficient Implementation of the Barrier Method} \label{sec:bar}

We are trying to solve \eqref{eq:b} for a particular value of $\eta$.  For each value of $\eta$, we can efficiently solve this problem using the Newton's method, as it is concave. First, we must compute the gradient (first derivative) and Hessian (second derivative) of the objective in \eqref{eq:b} with respect to $\Ca$, which requires that we write $n_t$ as a function of $\Ca_t$, which we can using \eqref{eq:nMC}. Then, we use these derivatives to choose the direction to ascend. The likelihood, gradient, and Hessian are, in matrix notation:

\begin{align}
\Lik &= \norm{\ve{F}-\Cav}^2_2 + \lambda \ve{1}' \ve{n} - \eta \log \ve{n}\\
&= \norm{\ve{F}-\Cav}^2_2 + \lambda \ve{1}' \ma{M} \Cav - \eta \log \ma{M} \Cav\\
\ve{g} &= -2(\ve{F}-\Cav) +\lambda \ve{1}' \ma{M} - \eta \ma{M}' \log (\ve{n}^{-1}) \label{eq:g}\\
\ma{H} &= 2\ma{I} + 2 \eta \ma{M}' \log(\ve{n}^{-2}) \ma{M} \label{eq:H}
\end{align}

\noindent where $\ma{I}$ is the identity matrix of appropriate size.  Now the key is that the Hessian is a tridiagonal matrix, which follows from the fact that $\ma{M}$ is bidiagonal.  Therefore, to compute the typical Newton step, update $\Cav$ as follows:

\begin{align}
\Cav \leftarrow \Cav + s \ve{d} 
\end{align}

\noindent where $\ve{d}=\ma{H}^{-1} \ve{g}$, which may be performed in $O(T)$ time using standard efficient algorithms (e.g., Matlab's $\backslash$), as opposed to the more general $O(T^3)$ time, due to the tridiagonal nature of $\ma{H}$.  The step size $s$ is typically taken to be unity, unless doing so violates any of the constraints.  For this problem, we require that $n_t>0$ for all $t$; therefore, we have:

\begin{table}[h]
\caption{Pseudocode for PPR}
\label{tab:bar}
\fbox{\begin{minipage}{1.0\linewidth}
This algorithm iterates several steps, as schematized by Figure \ref{fig:ppr_schem}. The algorithm is initialized with the fluorescence signal, which is the residual before iterating the following steps, $\ve{r}_0$:
\begin{itemize}
\item Compute the cross-correlation between the residual and the calcium kernel, 
$ \ve{r}_0 \otimes A e^{-t/\tau}. $
\item Let $t_{i+1}$ be the maximum of that cross-correlation,
$ t_{i+1} = \max_t \ve{r}_0 \otimes A e^{-t/\tau}. $
\item Convolve the calcium kernel with $t_i$ to generate the expected effect of a spike having occurred at that time step,
$ t_{i+1} \ast e^{-t/\tau}. $
\item Subtract the result of \eqref{eq:t_i} from the residual of the previous step to get the new residual,
$ r_{i+1} = r_i -  t_{i+1} \ast e^{-t/\tau} $
\item Compute the regularized sum of residual error, where $n_{i+1,t}$ is a vector of zeros with ones for each $t=t_i$,
$ \epsilon_{i+1} = \sum_t r_{i+1,t} + \lambda_t n_{i+1,t} $
\item If $r_{i+1} < r_i$, repeat.  Otherwise, let $\ve{n}_{ppr} = \ve{n}_i$.
\end{itemize}
\end{minipage}}
\end{table}

