Given such a model, our goal is to find the maximum \emph{a posteriori} (MAP) spike train, i.e., the most likely spike train, $\bn$ (where we use the shorthand notation $\bX=[X_1, \ldots, X_T]$, and $T$ is the final time step),  given the fluorescence measurements, $\bF$. %=[F_1,\ldots, F_T]$.
Thus, the objective function that we'd like to solve may be written as:

%\begin{subequations}
\begin{align}
\hnm &=  \ann P(\bn | \bF) = \ann P(\bF | \bn) P(\bn) \nonumber  \\
&= \argmax_{n_t \in \mathbb{N}_0 \forall t} \prod_{t=1}^T  P(F_t | C_t)  P(n_t) = \argmax_{n_t \in \mathbb{N}_0 \forall t} \sum_{t=1}^T \big( \log P(F_t | C_t) + \log P(n_t)\big)  \nonumber \\
&= \ann  \sum_{t=1}^T \bigg( \frac{1}{2 \sig^2}(F_t - \alpha C_t - \beta)^2  -  n_t \log \lam \Del + \log n_t! \bigg),   \label{eq:obj}
\end{align}
%\end{subequations}

\noindent where $\mathbb{N}_0 = \{0, 1, 2, \ldots\}$, and $C_t$ is implicitly a function of $n_t$.  Unfortunately, the computational complexity of solving Eq. \eqref{eq:obj} scales exponentially with $T$, i.e., is $O($e$^T)$, making Eq. \eqref{eq:obj} an NP-hard problem.  Thus, instead of an exact solution, we propose to make an approximation that reduces the complexity to be \emph{polynomial} in $T$.  In particular, we relax the assumption that we must have an integer number of spikes at any time step, by approximating the Poisson distribution with an exponential.  Note that this is a common approximation technique in the machine learning literature \cite{HastieFriedman01}, as it is the closest convex relaxation to its non-convex counterpart.  The constraint on $n_t$ in Eq. \eqref{eq:obj} is therefore relaxed from  $n_t \in \mathbb{N}_0$ to $n_t \geq 0$:

\begin{align} \label{eq:obj2}
\hnm &\approx \an  \sum_{t=1}^T \left( \frac{1}{2 \sig^2}(F_t -\alpha C_t - \beta)^2  +  n_t  \lam \Del \right).
\end{align}

While this convex relaxation makes the problem tractable, the ``sharp'' threshold imposed by the nonnegativity constraint prohibits the use of standard gradient ascent techniques \cite{CONV04}. We therefore take an ``interior-point'' (or ``barrier'') approach, in which we drop the sharp threshold, and add a barrier term, which must approach $-\infty$ as $n_t$ approaches zero (e.g., $-\log n_t$) \cite{CONV04}.  By iteratively reducing the weight of the barrier term, $\zzz > 0$, we are guaranteed to converge to the correct solution \cite{CONV04}, $\hnm$.  Thus, our goal is to efficiently solve:

\begin{align} \label{eq:eta}
\hbn_{\zzz} &= \argmin_{n_t \forall t}  \sum_{t=1}^T \left( \frac{1}{2 \sig^2}(F_t -\alpha C_t - \beta)^2  +  n_t  \lam \Del - \zzz \log(n_t) \right),
\end{align}

\noindent which is concave and may therefore be solved exactly and efficiently using a gradient ascent technique. To see how, we first note that   
% such as Newton-Raphson, which, na\"{i}ely, requires inverting the Hessian (second derivative), an $O(T^3)$ operation.  
%We have therefore reduced the computational burden from exponential to polynomial in $T$.   However, by utilizing the structure of Eq. \eqref{eq:trans}, we can devise an algorithm that further reduces computation burden from polynomial in $T$ to \emph{linear} in $T$.  In particular, we note here that 
spikes and calcium are related to one another via a simple linear transformation, namely, $n_t=f(C_t,C_{t-1})= C_t - \gamma C_{t-1}$, or, in matrix notation: 

\begin{align} \label{eq:M}
\ve{M} \bC = %- \bb=
\begin{bmatrix}
1 & 0  & 0 & \cdots & \cdots \\
1 & -\gamma & 0 & \cdots & \cdots \\
0 & 1 & -\gamma & 0 & \cdots  \\
\vdots & \vdots & \vdots & \vdots & \vdots  \\
0 & 0 & 0 & 1 & -\gamma
\end{bmatrix}
\begin{bmatrix}
C_1 \\ C_2 \\ \vdots \\ \vdots \\ C_T  
\end{bmatrix}
%-\begin{bmatrix}
%\nu/\rho \\ \nu/\rho \\ \vdots \\ \vdots \\ \nu/\rho 
%\end{bmatrix}
= 
\begin{bmatrix}
n_1 \\ n_2 \\ \vdots \\ \vdots \\ n_T
\end{bmatrix}
= \bn
\end{align}

\noindent where $\ve{M} \in \mathbb{R}^{T \times T}$ is a bidiagonal matrix,  and $\bb$, $\bC=[C_1,\ldots, C_T]$, and $\bn$ are $T$ dimensional column vectors. Given Eq. \eqref{eq:M}, we may rewrite Eq. \eqref{eq:eta} in terms of $\bC$:

%\begin{subequations}  
\begin{align} 
\hbC_{\zzz} &= \argmin_{C_t - \gamma C_{t-1} \geq 0 \forall t} \sum_{t=1}^{T} \left( \frac{1}{2 \sig^2}(F_t - \alpha C_t - \beta)^2  + (C_t - \gamma C_{t-1}) \lam \Del - \zzz \log(C_t - \gamma C_{-1} -\nu ) \right) \nonumber \\
&= \az  \frac{1}{2 \sig^2} \norm{\bF - \alpha \bC -\beta \ve{1} }^2 + \lam \Del (\bM \bC )' \ve{1}  - \zzz \log(\bM \bC)'\ve{1},  \label{eq:eta2}
\end{align}
%\end{subequations}

\noindent where $\bM \bC \geq \ve{0}$ indicates that every element of $\bM \bC$ is greater than or equal to zero, $'$ indicates transpose, and  $\ve{1}$ is a $T$ dimensional column vector, and $\log(\cdot)$ indicates an element-wise logarithm.  As \eqref{eq:eta2} is concave (it is identical to Eq. \eqref{eq:eta} with different notation), we may use any descent technique to find $\hbC_{\zzz}$.  We elect to use the Newton-Raphson approach: 

%, i.e., update $\widehat{\ve{C}}_{\zzz}$ by adding to it the solution to $\ve{H}\ve{C} = \ve{g}$, weighted by $0<s\leq1$, where $\ve{H}$ and $\ve{g}$ are the Hessian and gradient of the argument in \eqref{eq:goal2}.  The step size, $s$, ensures that the posterior converges, by enforcing an increase in the objective with each step. Therefore, we have: 

\begin{subequations} \label{eq:NR}
\begin{align}
\hbC_{\zzz} &\leftarrow \hbC_{\zzz} + s \bd \\
\bH \bd &= \bg \\
\ve{g} &= -\frac{\alpha}{\sig^2}(\bF -\alpha \hbC_{\zzz} - \beta) + \lam \Del \ve{M}' \ve{1} - \zzz \ve{M}' (\ve{M} \hbC_{\zzz})^{-1} \label{eq:g} \\
\ve{H} &= \frac{\alpha^2}{\sig^2} \ve{I} + \zzz \ve{M}' (\ve{M} \hbC_{\zzz})^{-2} \ve{M} \label{eq:H}
\end{align}
\end{subequations}

\noindent where $s$ is the step size, $\bd$ is the step direction, and $\bg$ and $\bH$ are the gradient (first derivative) and Hessian (second derivative) of the likelihood, respectively (the likelihood is the argument to be minimized in Eq. \eqref{eq:eta2}), and the exponents indicate element-wise operations. Note that we use ``backtracking linesearches'', meaning that for each iteration, we find the maximal $s$ that is between $0$ and $1$ and decreases the likelihood.

Typically, implementing Newton-Raphson requires inverting the Hessian, i.e., $\bd = \bH^{-1} \bg$, a computation consuming $O(T^3)$ time. Instead, because $\ve{M}$ is bidiagonal, the Hessian is tridiagonal, so the solution may be found in $O(T)$ time via standard banded Gaussian elimination techniques (which can be implemented efficiently in Matlab using $\bH \backslash \bg$). The resulting fast algorithm for solving the optimization problem in \eqref{eq:obj2} is the main result of this paper, i.e.  we may approximately infer the optimal solution  for models characterized by equations such as \eqref{eq:obs} and \eqref{eq:trans} in linear time, whereas an exact solution would require exponential time, and a na\"{i}ve approximate solution would require polynomial time.  


