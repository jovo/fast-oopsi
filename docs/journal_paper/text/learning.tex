In the above, we assumed that the parameters governing our model, $\vth=\{\balpha, \beta, \sig, \gam, \nu, \rho. \lam\} \in \ve{\Theta}$,  %=\Real^5_+$ (because all these parameters are necessarily non-negative)
were known. In general, however, these parameters may be estimated from the data. To find the maximum likelihood estimator for the parameters, $\hvth$, we must integrate over the unknown variable, $\zT$. However, integrating over all possible spike trains is typically approximated by Monte Carlo approaches, which is relatively slow. Thus, one often approximates: % the integral with the most likelihood estimate of $\zT$ \cite{??}, yielding:

%\begin{subequations} \label{eq:par}
\begin{align} \label{eq:par1}
\widehat{\bth} &= \argmax_{\bth \in \ve{\Theta}} \iint P[\bF|\bC, \bn, \bth] P[\bC, \bn | \bth] d\bn d\bC  \approx \argmax_{\bth \in \ve{\Theta}}P[\bF| \hCm, \hnm, \bth] P[\hCm, \hnm | \bth]
\end{align}

\noindent where $\hnm=[\hn_1, \ldots, \hn_T]$ is estimate of $\hnm$ from the inference algorithm described above ($\hCm$ is defined similarly). The approximation in \eqref{eq:par} is good whenever the likelihood is very peaky, meaning that most of the mass is around the MAP sequence.\footnote{The approximation in \eqref{eq:par} may be considered a first-order Laplace approximation}  In particular, for state-space models, one often approximates the integral in \eqref{eq:par} with the Viterbi path, i.e., the MAP path of the hidden states \cite{Rabiner89}. Finding the Viterbi path is often far easier than solving the integral in \eqref{eq:par}, as in the case here.  Due to the state space nature of the above model (Eqs \eqref{eq:obs} and \eqref{eq:trans}), the optimization in Eq \eqref{eq:par1} simplifies significantly.  More specifically, we can write the argument from the left-hand-side of Eq \eqref{eq:par1} as a product of terms that we have defined in our model:

 \begin{align} \label{eq:par2}
P[\bF| \hCm, \hnm, \bth] P[\hCm, \hnm | \bth] &= \prod_{t=1}^T P[F_t | \hC_t, \balpha, \beta,\sig] P[\hC_t | \hC_{t-1}, \hn_t, \gamma] P[\hn_t | \lam]
\end{align}

\noindent Thus, this likelihood is jointly concave in all the parameters.  To solve for $\{\balpha,\beta,\sig\}$, we solve:

\begin{align} 
\{\halpha, \hbeta, \hsig\} &= \argmax_{\balpha, \beta, \sig > 0} \sum_{t=1}^T \log P[F_t | \hC_t, \balpha, \beta, \sig] \nonumber \\
&=  \argmax_{\balpha, \beta, \sig > 0} -\frac{T}{2} \log (2 \pi \sig^2)  - \frac{1}{2\sig^2} \sum_{t=1}^T (F_t - \balpha \hC_{t} - \beta )^2 \nonumber \\
&=   \argmax_{\balpha, \beta, \sig > 0} -\frac{T}{2} \log (2 \pi \sig^2)  - \frac{1}{2\sig^2} \norm{\bF- \hbX \ve{\eta}}^2 \label{eq:lik2}
%P[\bF | \hCm, \balpha, \beta, \sig) &= \prod_{t=1}^T P[F_t | \hC_t, \balpha, \beta, \sig) = \prod_{t=1}^T \frac{1}{\sqrt{2 \pi \sig^2}} \exp \left\{-\frac{1}{2\sig^2} (F_t - \balpha \hC_t - \beta )^2\right\}, \label{eq:lik}% \nonumber \\
%&= \prod_{t=1}^T \frac{1}{\sqrt{2 \pi \sig^2}} \exp \left\{-\frac{1}{2\sig^2} \big[F_t - \balpha (\gamma \hC_{t-1} + n_t) - \beta \big]^2\right\} \label{eq:lik}
\end{align}

%\noindent which follows from Eq \eqref{eq:trans}. Taking the log of both sides yields:
%
% because $\hC_t$ is deterministic given $\hn_t$. Considering for a moment, only the quadratic term in Eq \eqref{eq:lik}, the terms can be reorganized:
%
%\begin{align}
%F_t - \balpha (\gamma \hC_{t-1} + \nu + \rho \hn_t) - \beta &= F_t - (\balpha \gamma) \hC_{t-1} - (\balpha \nu + \beta) - (\balpha \rho) \hn_t 
%\end{align}
%
%\noindent which emphasizes the over-parameterization in our model.  Thus, without loss of generality, we can assume that $\balpha=1$ and $\beta=0$. Taking logs in Eq \eqref{eq:lik} then yields:
%
%\begin{align} 
%\log P[\bF | \hnm, \bth) &=  -\frac{T}{2} \log (2 \pi \sig^2)  - \frac{1}{2\sig^2} \sum_{t=1}^T (F_t - \balpha \hC_{t} - \beta )^2 \nonumber \\
%&=  -\frac{T}{2} \log (2 \pi \sig^2)  - \frac{1}{2\sig^2} \norm{\bF- \hbX \ve{\eta}}^2
% \label{eq:lik2}
%\end{align}

\noindent where  $\ve{\eta}=[\balpha, \, \beta]$  and $\hbX_t=[\hC_{t}, \, 1]$.  Thus, solving for $\{\halpha,\hbeta\}$, is a constrained quadratic optimization problem, which can be solved efficiently using standard tools, such as Matlab's \texttt{quadprog}.  The variance may then be solved for analytically:

\begin{align} 
\widehat{\sig}^2 &= \frac{1}{T} \norm{\bF - \hbX \widehat{\ve{\eta}}}_2^2 \label{eq:sig}
\end{align}

\noindent Similarly, taking the log of the prior term, $P[\hnm | \bth )$ yields:

\begin{align} \label{eq:prior}
\log P[\hnm | \bth] =  \sum _{t=1}^T \log(\lam \Del)  -\lam \Del \hn_t =T (\lam \Del) - \lam \Del \hnm' \ve{1} 
\end{align}

\noindent Plugging Eqs \eqref{eq:lik2} and \eqref{eq:prior} back into Eq \eqref{eq:par1}, and noting that log is a monotonic function, we have:


%Given Eqs. \eqref{eq:obs} and \eqref{eq:trans}, we can write the above likelihood function as:

\begin{align} 
%\hvth &=\argmax_{\vth \in \ve{\Theta}}  \sum_{t=1}^T \left(- \frac{1}{2} \log (2\pi\sig^2)-\frac{1}{2\sig^2} (F_t - \gamma \hC_{t-1} -\nu  - \rho \hn_t)^2 \right) + \sum_{t=1}^T \big(\log (\lam \Del) -  \lam \Del n_t\big) \nonumber \\
\hvth &=\argmax_{\vth \in \ve{\Theta}} - \frac{T}{2} \log (2\pi\sig^2) - \frac{1}{2\sig^2} \norm{\bF - \hbX \ve{\eta}}_2^2 + T \log (\lam \Del) - \lam \Del \hnm' \ve{1} \label{eq:par} 
%&=\argmin_{\vth}  \frac{T}{2} \log (2\pi\sig^2)  +  \frac{1}{2\sig^2} \norm{\bY + \ve{\eta} \bX}_2^2 + T \log (\lam \Del) - \lam \Del \bn' \ve{1} \label{eq:par} 
\end{align}
%\end{subequations}

\noindent %where %$\bY=\bF- \hnm$, 
%$\bX=[\hCm, \,  \ve{1},\,  \hnm]'$, $\ve{\eta}=[\gamma, \nu, \rho]'$, and $\hCm=[\hC_1, \hC_2, \ldots, \hC_{T-1}]'$ is the inferred calcium concentration using the algorithm described in Section \ref{sec:inf} (and $\hnm$ is defined in a similar fashion). Note that had we not fixed $\balpha=1$ and $\beta=0$, then we would have had $\ve{\eta}=[\balpha \gamma, \balpha \nu + \beta, \balpha \nu]$, and then these parameters would not have been identifiable.  
Estimating the parameters then separates into three log-concave problems.  In particular, solving for $\widehat{\ve{\eta}}$ is simply:

\begin{align} 
\widehat{\ve{\eta}} &= \argmin_{\ve{\eta},\: 0< \eta_1<1} \frac{1}{2} \norm{\bF - \hbX \ve{\eta}}_2^2 
=  \argmin_{\ve{\eta},\: 0< \eta_1<1} \frac{1}{2} \ve{\eta}' \bQ  \ve{\eta} - \bL'  \ve{\eta} \label{eq:m} 
\end{align}

\noindent where $\bQ=\hbX' \hbX$, and $\bL=\hbX' \bF$, and the constraint, $0<\eta_1<1$, ensures that the time constant is both non-negative and finite.  Eq \eqref{eq:m} may be solved using standard quadratic optimization tools, such as Matlab's \texttt{quadprog}.  

Unfortunately, the recursive nature of our algorithm makes solving for $\hbeeta$ in this manner unidentifiable. In particular, because both $\hbX$ and $\beeta$ can scale and shift $\bC$, alternating between inferring $\hbX$ and $\hbeeta$ does not converge (data not shown). However, this is not particularly problematic, because the fluorescence measurements are in arbitrary units. This unit arbitrariness suggests that $\bF$ may be scaled and shift without loss of generality. The prior term and non-negativity constraint on $\bn$, however, imposes small costs associated with scaling and shifting $\bn$ and $\bC$.  We therefore estimate $\beeta$ from the raw fluorescence trace.  More specifically, to estimate $\gamma$, we manually find a fluorescence sequence that seem to be decaying, estimate the time constant $\tau$, and let $\gamma=1-\Del/\tau$.  For $\nu$, we manually find a fluorescence sequence that seems to be near baseline, label it $[F_s,\ldots, F_t]$, and let $\nu=\frac{1}{t-s}\sum_{u=s}^t F_u$.  Finally, we estimate $\rho$ by letting it be equal to what we manually determine it be, by eye.  In practice, we have found that by first scaling and shifting $\bF$ to be between $0$ and $1$, subsequent traces from the same or different cells have very similar values for $\beeta$, and therefore, these parameters need not be tweaked much.  Furthermore, the effective signal-to-noise ratio (SNR) of the inferred spike train does not depend strongly on these parameters, in agreement with previous results \cite{YaksiFriedrich06}.  The other parameters, $\sig$ and $\lam$, can be solved for analytically:

\begin{align} 
\widehat{\sig}^2 &= \frac{1}{T} \norm{\bF - \hbX \widehat{\ve{\eta}}}_2^2 \label{eq:sig2}\\
\widehat{\lam} &=  \frac{1}{T \Del} \hnm' \ve{1} \label{eq:lam}
\end{align}

%\noindent where Eq. \eqref{eq:m} may be solved using standard linearly constrained quadratic programming (the constraint ensures that the time constant is both non-negative and finite), and the other two may be solved analytically. Importantly, when $\bn$ is unknown, $\widehat{\ve{\eta}}$ is not identifiable, as $\bY$ and $\ve{\eta}$ can both modify the offset and scale of $\bn$. Thus, we estimate $\ve{\eta}$ \emph{before} doing the analysis, and then hold it fixed throughout.  

%. Because the noise is Gaussian, $a$ and $\beta$ may be estimated using standard constrained quadratic programming:
% 
%\begin{subequations}
%\begin{align} \label{eq:m}
%\{\ha,\hbeta\}% &= \argmax_{a,\beta>0}\frac{1}{\sqrt{2 \pi} \sig} \exp \left\{ -\frac{1}{2} \left(\frac{F_t - a \widehat{C}_{t-1} - \widehat{n}_t - \beta}{\sig}\right)^2\right\} 
%= \argmin_{a,\beta>0} \sum_{t=1}^T (F_t - a \widehat{C}_{t-1} - \widehat{n}_t -\beta )^2 \\
%\widehat{\ve{\eta}} &= \argmin_{\bm>0} \norm{\bY + \bm \bX}_2^2. % = \argmin_{\bm>0}  \bm' \bX' \bX \bm  - 2 \bY' \bX \bm 
%\end{align}
%\end{subequations}
%
% Similarly, because $\varepsilon_t$ is Gaussian, we compute $\widehat{\sig}$ analytically using: 
%
%\begin{align}
%\widehat{\sig}^2 = \frac{1}{T} \norm{\bY + \widehat{\ve{\eta}} \bX}_2^2.
%\end{align}
%
%\noindent The parameter for the prior term is given by:
%
%\begin{align}
%\widehat{\lam} = \argmax_{\lam} P[\widehat{\ve{n}} | \lam) =  \argmax_{\lam} \log P[\widehat{\ve{n}} | \lam) =  \frac{T}{ \Del \bn' \ve{1}}
%\end{align}
%
%\noindent which follows from assuming that $p(\zT | \lT)$ is exponential in \eqref{eq:approx}.


