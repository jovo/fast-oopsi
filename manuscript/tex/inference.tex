%\paragraph{FAst Non-negative Deconvolution (\foopsi) Filter}

Our goal here is to develop an algorithm to efficiently approximate $\hbn$, the most likely spike train, given the fluorescence trace.  %The main contribution of this work demonstrating that we can substitute the Poisson distribution with an exponential distribution, reducing computational complexity from $O(k^T)$ to $O(T)$.  Formally, we replace Eq. \eqref{eq:C} with:
%
%\begin{align} \label{eq:C2}
%	C = \gam C_{t-1} + n_t, \qquad n_t \overset{iid}{\sim} \text{Exponential}(\lam \Del)
%\end{align}
% 
%\noindent where the only difference between Eq. \eqref{eq:C} and Eq. \eqref{eq:C2} is the distribution on $n_t$.  Plugging this change into Eq. \eqref{eq:naht3} yields
%%, by replacing the Poisson distribution on $\bn$ with an exponential distribution. Plugging this approximation into Eq. \eqref{eq:nhat4} yields:
By letting Poisson$(n_t; \lam\Del) \approx$ Exponential$(n_t; \lam\Del)$, Eq. \eqref{eq:logobj1} becomes:

\begin{align} \label{eq:obj2}
\hbn &\approx \argmin_{n_t>0 \, \forall t}  \sum_{t=1}^T \bigg( \frac{1}{2 \sig^2}(F_t - \alpha(C_t + \beta))^2  + n_t \lam \Del \bigg) %, \qquad n_t \overset{iid}{\sim} \text{Exponential}(\lam \Del) %\an  \sum_{t=1}^T \left( \frac{1}{2 \sig^2}(\norm{\bF_t -\balpha (C_t + \beta)}^2  +  n_t  \lam \Del \right).
\end{align}

\noindent where the constraint on $n_t$ has been relaxed from  $n_t \in \mathbb{N}_0$ to $n_t \geq 0$ (since the exponential distribution can yield any non-negative number), and we replaced $\max$ with $\min$ and inverted the signs.  Note that this is a common approximation technique in the machine learning literature \cite{TBF01}, as the exponential distribution is the closest convex relaxation to its non-convex counterpart, the Poisson distribution. While this convex relaxation makes the problem tractable, the ``sharp'' threshold imposed by the non-negativity constraint prohibits the use of standard gradient ascent techniques \cite{CONV04}. We therefore take an ``interior-point'' (or ``barrier'') approach, in which we drop the sharp threshold, and add a barrier term, which must approach $-\infty$ as $n_t$ approaches zero (e.g., $-\log n_t$) \cite{CONV04}.  By iteratively reducing the weight of the barrier term, we are guaranteed to converge to the correct solution \cite{CONV04}.  Thus, our goal is to efficiently solve:
\begin{align} \label{eq:eta}
\hbn_{\zzz} &= \argmin_{n_t \forall t}  \sum_{t=1}^T \left( \frac{1}{2 \sig^2}(F_t - \alpha(C_t + \beta))^2  +  n_t  \lam \Del - \zzz \log(n_t) \right),
\end{align}
Since spikes and calcium are related to one another via a simple linear transformation, namely, $n_t= \del (C_t-\gam C_{t-1})$, where $\del=\tau/\Del$ and $\gam=1-\Del/\tau$.  Dropping $\del$ (because we have a free scale term) we may rewrite Eq. \eqref{eq:eta} in terms of $\bC$:

\begin{align} 
\hbC_{\zzz} &= \argmin_{C_t - \gamma C_{t-1} \geq 0 \forall t} \sum_{t=1}^{T} \left( \frac{1}{2 \sig^2} (F_t -\alpha (C_t + \beta))^2  + (C_t - \gamma C_{t-1}) \lam \Del - \zzz \log(C_t - \gamma C_{-1}) \right). \label{eq:eta2}
\end{align}

\noindent The concavity of Eq. \eqref{eq:eta2} facilitates utilizing any number of techniques guaranteed to find the global optimum.  The fact that the argument of Eq. \eqref{eq:eta2} is twice differentiable, allows for the use of the Newton-Raphson technique, which is typically more efficient than only incorporating the gradient \cite{CONV04}. First, we rewrite Eq. \eqref{eq:eta2} in matrix notation.  Note that we can write $\bM \bC = \bn$:

\begin{align} \label{eq:M}
\ve{M} \bC = %- \bb=
\begin{bmatrix}
1 & 0  & 0 & \cdots & \cdots \\
1 & -\gamma & 0 & \cdots & \cdots \\
0 & 1 & -\gamma & 0 & \cdots  \\
\vdots & \vdots & \vdots & \vdots & \vdots  \\
0 & 0 & 0 & 1 & -\gamma
\end{bmatrix}
\begin{bmatrix}
C_1 \\ C_2 \\ \vdots \\ \vdots \\ C_T  
\end{bmatrix}
%-\begin{bmatrix}
%\nu/\rho \\ \nu/\rho \\ \vdots \\ \vdots \\ \nu/\rho 
%\end{bmatrix}
= 
\begin{bmatrix}
n_1 \\ n_2 \\ \vdots \\ \vdots \\ n_T
\end{bmatrix}
= \bn
\end{align}

\noindent where $\ve{M} \in \mathbb{R}^{T \times T}$ is a bidiagonal matrix.  Then, leting $\ve{1}$ be a $T$ dimensional column vector and $\blam=\lam \Del \ve{1}\T$, we obtain: %, and $\log(\cdot)$ indicates an element-wise logarithm.  Note that Eq. \eqref{eq:eta3} follows from writing $\bn$ in terms of $\bM$ and $\bC$:
% 
%Importantly, the state-space nature of this problem yields a particularly efficient approach. Specifically, note that the Hessian is \emph{tridiagonal}, which is clear upon rewriting \eqref{eq:eta2} in matrix notation:

\begin{align} 
\hbC_{\zzz} %&= \argmin_{C_t - \gamma C_{t-1} \geq 0 \forall t} \sum_{t=1}^{T} \left( \frac{1}{2 \sig^2}\norm{\bF_t -\balpha (C_t + \beta)}^2  + (C_t - \gamma C_{t-1}) \lam \Del - \zzz \log(C_t - \gamma C_{-1} -\nu ) \right) \nonumber \\
&= \az  \frac{1}{2 \sig^2} \norm{\bF - \alpha (\bC +\beta)}^2 + (\bM \bC )\T \blam  - \zzz \log(\bM \bC)\T\ve{1},  \label{eq:eta3}
\end{align}

\noindent where $\bM \bC \geq \ve{0}$ indicates that every element of $\bM \bC$ is greater than or equal to zero, $\T$ indicates transpose, and $\log(\cdot)$ indicates an element-wise logarithm.  %Note that Eq. \eqref{eq:eta3} follows from writing $\bn$ in terms of $\bM$ and $\bC$:
%
% such as Newton-Raphson, which, na\"{i}ely, requires inverting the Hessian (second derivative), an $O(T^3)$ operation.  
%We have therefore reduced the computational burden from exponential to polynomial in $T$.   However, by utilizing the structure of Eq. \eqref{eq:trans}, we can devise an algorithm that further reduces computation burden from polynomial in $T$ to \emph{linear} in $T$.  In particular, we note here that 
%, or, in matrix notation: 


%\noindent and $\ve{M} \in \mathbb{R}^{T \times T}$ is a bidiagonal matrix. Given Eq. \eqref{eq:M}, we may rewrite Eq. \eqref{eq:eta} only in terms of $\bC$ (i.e., not $\bn$):
%
%\begin{subequations}  
%\begin{align} 
%\hbC_{\zzz} &= \argmin_{C_t - \gamma C_{t-1} \geq 0 \forall t} \sum_{t=1}^{T} \left( \frac{1}{2 \sig^2}\norm{\bF_t -\balpha (C_t + \beta)}^2  + (C_t - \gamma C_{t-1}) \lam \Del - \zzz \log(C_t - \gamma C_{-1} -\nu ) \right) \nonumber \\
%&= \az  \frac{1}{2 \sig^2} \norm{\bF - \balpha (\trans{\bC} +\beta+\ve{1}\T)}^2 + \lam \Del (\bM \bC )\T \ve{1}  - \zzz \log(\bM \bC)\T\ve{1},  \label{eq:eta2}
%\end{align}
%\end{subequations}
% 
Now, when using Newton-Raphson to ascend a gradient, we iteratively compute both the gradient, $\bg$ (first derivative), and Hessian, $\bH$ (second derivation), of the argument to be minimized, with respect to the variables of interest ($\bC_z$ here).  Then, we update our estimate, using $\bC_z \leftarrow \bC_z + s \bd$, where $s$ is the step size, and solving $\bH \bd = \bg$ provides $\bd$, the direction.  From Eq. \eqref{eq:eta3}, we therefore need:

% Thus, a little bit of calculus yields our update algorithm for $\bC_z$:
% 
%As \eqref{eq:eta2} is concave (it is identical to Eq. \eqref{eq:eta} with different notation), we may use any descent technique to find $\hbC_{\zzz}$.  We elect to use the Newton-Raphson approach: 

%, i.e., update $\widehat{\ve{C}}_{\zzz}$ by adding to it the solution to $\ve{H}\ve{C} = \ve{g}$, weighted by $0<s\leq1$, where $\ve{H}$ and $\ve{g}$ are the Hessian and gradient of the argument in \eqref{eq:goal2}.  The step size, $s$, ensures that the posterior converges, by enforcing an increase in the objective with each step. Therefore, we have: 

\begin{subequations} \label{eq:NR}
\begin{align}
%\hbC_{\zzz} &\leftarrow \hbC_{\zzz} + s \bd \\
%\bH \bd &= \bg \\
\ve{g} &= -\frac{\alpha}{\sig^2}(\bF -\alpha({\hbC\T}_{\zzz} + \beta)) + \ve{M}\T\blam - \zzz \ve{M}\T (\ve{M} \hbC_{\zzz})^{-1} \label{eq:g} \\
\ve{H} &= \frac{\alpha^2}{\sig^2} \ve{I} + \zzz \ve{M}\T (\ve{M} \hbC_{\zzz})^{-2} \ve{M} \label{eq:H}
\end{align}
\end{subequations}

\noindent where %$s$ is the step size, $\bd$ is the step direction, and $\bg$ and $\bH$ are the gradient (first derivative) and Hessian (second derivative) of the argument in Eq. \eqref{eq:eta3} with respect to $\bC$, respectively, and 
the exponents indicate element-wise operations. To compute $s$, we use ``backtracking linesearches'', meaning that we find the maximal $s$ that is (a) between $0$ and $1$ and (b) decreases the likelihood.

Typically, implementing Newton-Raphson requires inverting the Hessian, i.e., $\bd = \bH^{-1} \bg$, a computation that scales \emph{cubically} with $T$, i.e., requires approximately $T^3$ operations. Already, this would be a drastic improvement over the most efficient algorithm assuming Poisson spikes, which require $k^T$ operations (where $k$ is the maximum number of spikes per frame).  Here, because $\ve{M}$ is bidiagonal, the Hessian is tridiagonal, the solution may be found in approximately $T$ operations, via standard banded Gaussian elimination techniques (which can be implemented efficiently in Matlab using $\bH \backslash \bg$). In other words, the above approximation and inference algorithm reduces computations from exponential time to \emph{linear} time. We refer to this as the Fast Optimal OPtical Spike Inference (\foopsi) filter. 